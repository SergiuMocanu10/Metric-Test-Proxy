{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression for Code-Quality Metrics\n",
    "\n",
    "The following is a demonstration of the use of machine learning prediction model (__Logistic Regression__) in order to establish if a correlation exists between *textual-similarity metrics* and the desired behavior of LLM-generated scripts. The dataset that was chosen for this research is [HumanEval+](https://github.com/evalplus/evalplus/releases/tag/v0.1.0), a set of 2.9 million LLM-generated Python scripts.\n",
    "\n",
    "&NewLine;\n",
    "Note: The first two steps of the experimental protocol -- functionality test and textual-metric measurement -- are not meant to be executed in this notebook due to the long duration of the process. Instead, we will use the results obtained during the research of this subject. The downloaded .zip file is to be extracted right outside the root directory of the project: https://filesender.renater.fr/?s=download&token=e2d853f8-a321-4f7a-834e-003eb0ce6356\n",
    "\n",
    "\n",
    "## Experimental Protocol\n",
    "The first step is to test the AI-script against the set of tests found in the canonical, human-made implementation of HumanEval tasks:"
   ],
   "id": "df1ea548089f3189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T08:42:20.646473Z",
     "start_time": "2025-05-22T08:42:20.613214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(marker='requirements.txt'):\n",
    "    path = Path().resolve()\n",
    "    for parent in [path] + list(path.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    raise FileNotFoundError(f\"Could not find {marker} in any parent directories\")\n",
    "\n",
    "project_root_parent = find_project_root().parent\n",
    "\n",
    "exp_data_path = os.path.join(project_root_parent, 'metric_exp_data')\n",
    "\n",
    "code_path = os.path.join(exp_data_path, 'code')\n",
    "results_path = os.path.join(exp_data_path, 'exp_results')\n",
    "\n",
    "def get_functionality_test_path(dataset_name):\n",
    "\n",
    "    target_path = os.path.join(results_path, dataset_name, 'functionality_tests')\n",
    "    return target_path\n",
    "\n",
    "\n",
    "def get_ai_code_path(dataset_name):\n",
    "    target_path = os.path.join(code_path, dataset_name)\n",
    "    return target_path\n",
    "\n",
    "\n",
    "def get_metric_results_path(dataset_name):\n",
    "    target_path = os.path.join(results_path, dataset_name, 'metrics_calc')\n",
    "    return target_path\n",
    "\n",
    "\n",
    "def get_logreg_results_path(dataset_name):\n",
    "    target_path = os.path.join(results_path, dataset_name, 'metrics_logreg')\n",
    "    return target_path\n",
    "\n",
    "'''\n",
    "Auxiliary functions used in the functionality testing of AI-generated code\n",
    "'''\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    # A sorting key used to sort strings in a length-lexicographic order (length and alphabetical order)\n",
    "    return len(s), s\n",
    "\n",
    "\n",
    "def code_cleanup(script, remove_assert=False, remove_exit=False):\n",
    "    # Function that removes any unnecessary components of a given script (comments & tests), leaving only the code lines\n",
    "\n",
    "    # Removing the test component of HumanEval implementation following 'METADATA' information\n",
    "    if 'METADATA' in script:\n",
    "        script = script.split('METADATA', 1)[0]\n",
    "    elif 'def check(candidate)' in script:\n",
    "        script = script.split('def check(candidate)', 1)[0]\n",
    "\n",
    "    script_lines = script.splitlines()\n",
    "\n",
    "    multi_line_comment = False\n",
    "    comment_index = []\n",
    "    assert_index = []\n",
    "    empty_line_index = []\n",
    "    exit_line_index = []\n",
    "\n",
    "    for index, line in enumerate(script_lines):\n",
    "\n",
    "        # Indexing any assert statement\n",
    "        if remove_assert and 'assert' in line and line[0] == 'a':\n",
    "            assert_index.append(index)\n",
    "            continue\n",
    "\n",
    "        if remove_exit and 'exit(' in line:\n",
    "            exit_line_index.append(index)\n",
    "            continue\n",
    "\n",
    "        if not multi_line_comment:\n",
    "            if '#' in line:\n",
    "                # Indexing single-line comments\n",
    "                if line.strip()[0] == '#':\n",
    "                    comment_index.append(index)\n",
    "                # Removing comment component of the line\n",
    "                else:\n",
    "                    cleaned_up_line = line.split('#', 1)[0]\n",
    "                    script_lines[index] = cleaned_up_line\n",
    "                continue\n",
    "\n",
    "            # Indexing the first line of multi-line comments\n",
    "            if '\"\"\"' in line or \"'''\" in line:\n",
    "                comment_index.append(index)\n",
    "                if line.count('\"\"\"') == 1 or line.count(\"'''\") == 1:\n",
    "                    multi_line_comment = True\n",
    "                continue\n",
    "\n",
    "        # Adding indexes for multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' not in line and \"'''\" not in line):\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing the last line of multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' in line or \"'''\" in line):\n",
    "            multi_line_comment = False\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing new lines and blank lines\n",
    "        if len(line) == 0 or line.isspace():\n",
    "            empty_line_index.append(index)\n",
    "            continue\n",
    "\n",
    "    # Merging indexes for comments, empty lines and assert statements\n",
    "    [comment_index.extend(indexes) for indexes in (empty_line_index, assert_index, exit_line_index)]\n",
    "\n",
    "    # Removing all the unnecessary parts of code\n",
    "    for index in sorted(comment_index, reverse=True):\n",
    "        del script_lines[index]\n",
    "\n",
    "    # Concatenating the list of script lines\n",
    "    clean_script = '\\n'.join(script_lines)\n",
    "    return clean_script\n",
    "\n",
    "\n",
    "def extract_checker(script):\n",
    "    # Function that extracts the test component of HumanEval implementations\n",
    "\n",
    "    # Extracting the 'checker' part of the HumanEval implementation\n",
    "    extracted_checker = script.split('def check(', 1)[1]\n",
    "    res = 'def check(' + extracted_checker\n",
    "\n",
    "    list_lines = res.split('\\n')\n",
    "\n",
    "    del_index = []\n",
    "\n",
    "    # Indexing empty lines, comments and useless asserts\n",
    "    for index, line in enumerate(list_lines):\n",
    "        if (len(line) == 0\n",
    "                or line.isspace()\n",
    "                or '#' in line\n",
    "                or 'assert True' in line):\n",
    "            del_index.append(index)\n",
    "\n",
    "    for index in reversed(del_index):\n",
    "        del list_lines[index]\n",
    "\n",
    "    res = '\\n'.join(list_lines)\n",
    "    return res\n",
    "\n",
    "'''\n",
    "Main function that tests the correct functionality of all AI-scripts\n",
    "'''\n",
    "def test_impl_functionality(target_dataset):\n",
    "    \"\"\"\n",
    "    Function that takes the AI-generated implementations and tests their correct functionality against the tests from\n",
    "    the HumanEval implementation\n",
    "\n",
    "    The result is saved locally in json files\n",
    "    \"\"\"\n",
    "    ai_code_path = get_ai_code_path(target_dataset)\n",
    "    humaneval_baseline_path = os.path.join(code_path, 'humaneval_baseline')\n",
    "    funct_test_path_prefix = get_functionality_test_path(target_dataset)\n",
    "\n",
    "    list_humaneval_scripts = sorted(os.listdir(humaneval_baseline_path))\n",
    "\n",
    "    exp_continuation_started = False\n",
    "\n",
    "    test_file_write_counter = 50\n",
    "\n",
    "    # Experiment-resumption mechanism\n",
    "    if os.path.exists(funct_test_path_prefix):\n",
    "        test_file_exists = True\n",
    "\n",
    "        # Obtaining the starting point of exp-resumption\n",
    "        list_models = sorted(os.listdir(funct_test_path_prefix))\n",
    "        last_tested_model = list_models[-1]\n",
    "        last_model_path = os.path.join(funct_test_path_prefix, last_tested_model)\n",
    "\n",
    "        list_model_temperatures = sorted(os.listdir(last_model_path))\n",
    "        last_tested_temperature = list_model_temperatures[-1]\n",
    "        tasks_folder_path = os.path.join(last_model_path, last_tested_temperature)\n",
    "\n",
    "        list_tested_tasks = sorted(os.listdir(tasks_folder_path), key=custom_sort_key)\n",
    "        last_task_name = list_tested_tasks[-1]\n",
    "        last_task_path = os.path.join(tasks_folder_path, last_task_name)\n",
    "        with open(last_task_path, 'r') as f:\n",
    "            dict_test = json.load(f)\n",
    "            if 'test_complete' in dict_test.keys() and dict_test['test_complete']:\n",
    "                last_task_nb = len(list_tested_tasks)\n",
    "                last_task_name = f'HumanEval_{last_task_nb}.json'\n",
    "                script_starting_index = 0\n",
    "            else:\n",
    "                script_starting_index = len(dict_test.keys())-1\n",
    "\n",
    "        model_name_and_temp = f'{last_tested_model}_{last_tested_temperature}'\n",
    "        list_models = sorted(os.listdir(ai_code_path))\n",
    "        model_temp_starting_index = list_models.index(model_name_and_temp)\n",
    "\n",
    "        task_starting_index = int(last_task_name.split('_')[1].strip('.json'))\n",
    "    \n",
    "    else:\n",
    "        test_file_exists = False\n",
    "        script_starting_index = task_starting_index = model_temp_starting_index = 0\n",
    "\n",
    "    list_models = sorted(os.listdir(ai_code_path))\n",
    "    for model_index in range(model_temp_starting_index, len(list_models)):\n",
    "        model_name_and_temp = list_models[model_index]\n",
    "        model_path = os.path.join(ai_code_path, model_name_and_temp)\n",
    "\n",
    "        print(f'Testing model: {model_name_and_temp}')\n",
    "\n",
    "        list_tasks = sorted(os.listdir(model_path), key=custom_sort_key)\n",
    "\n",
    "        for task_index in range(task_starting_index, len(list_tasks)):\n",
    "            # Skipping Task_145 due to lack of AI-code that accomplishes the said task\n",
    "            if task_index < 145:\n",
    "                task_number = task_index\n",
    "            else:\n",
    "                task_number = task_index + 1\n",
    "\n",
    "            task_name = f'HumanEval_{task_number}'\n",
    "            model_name = model_name_and_temp.split('_')[0]\n",
    "            model_temp = model_name_and_temp[-8:]\n",
    "\n",
    "            print(f'Testing task: {task_name}')\n",
    "\n",
    "            test_file_path = os.path.join(funct_test_path_prefix, model_name, model_temp, f'{task_name}.json')\n",
    "            if os.path.exists(test_file_path):\n",
    "                with open(test_file_path, 'r') as f:\n",
    "                    dict_test = json.load(f)\n",
    "            else:\n",
    "                dict_test = {'test_complete': False}\n",
    "\n",
    "            test_folder_path = test_file_path.rpartition('/')[0]\n",
    "            if not os.path.exists(test_folder_path):\n",
    "                os.makedirs(test_folder_path)\n",
    "\n",
    "            # Recovering the HumanEval per-task functionality tests\n",
    "            humaneval_file_name = list_humaneval_scripts[task_index]\n",
    "            humaneval_file_path = os.path.join(humaneval_baseline_path, humaneval_file_name)\n",
    "\n",
    "            humaneval_content = open(humaneval_file_path, 'r').read()\n",
    "\n",
    "            checker = extract_checker(humaneval_content)\n",
    "\n",
    "            generated_scripts_path = os.path.join(model_path, task_name)\n",
    "            list_generated_scripts = sorted(os.listdir(generated_scripts_path), key=custom_sort_key)\n",
    "\n",
    "            for script_index in range(script_starting_index, len(list_generated_scripts)):\n",
    "                # Cleaning and merging the LLM-generated script with the HumanEval functionality tests\n",
    "                script_name = list_generated_scripts[script_index]\n",
    "                script_path = os.path.join(generated_scripts_path, script_name)\n",
    "                script_content = open(script_path, 'r').read()\n",
    "                cleaned_script = code_cleanup(script_content, remove_exit=True)\n",
    "\n",
    "                merged_code = cleaned_script + '\\n\\n' + checker\n",
    "\n",
    "                dict_test[script_name] = {}\n",
    "\n",
    "                # Executing the merged script in a separate subprocess and stocking the result of the functionality test\n",
    "                try:\n",
    "                    subprocess.run(\n",
    "                        ['python', '-c', merged_code],\n",
    "                        stderr=subprocess.PIPE,\n",
    "                        timeout=2,\n",
    "                        check=True\n",
    "                    )\n",
    "\n",
    "                    dict_test[script_name]['successful'] = True\n",
    "\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    dict_test[script_name]['successful'] = False\n",
    "                    dict_test[script_name]['error_type'] = 'TimeOut'\n",
    "\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    dict_test[script_name]['successful'] = False\n",
    "\n",
    "                    error_name_and_message = e.stderr.decode().split('\\n')[-2]\n",
    "\n",
    "                    if 'AssertionError' in error_name_and_message:\n",
    "                        dict_test[script_name]['error_type'] = 'AssertionError'\n",
    "\n",
    "                    elif ':' in error_name_and_message:\n",
    "                        error_name = error_name_and_message.split(':')[0]\n",
    "                        error_message = error_name_and_message.split(':')[1].strip()\n",
    "                        dict_test[script_name]['error_type'] = error_name\n",
    "                        dict_test[script_name]['error_message'] = error_message\n",
    "\n",
    "                    else:\n",
    "                        dict_test[script_name]['error_type'] = error_name_and_message\n",
    "\n",
    "                # Writing the results in a json file every 50 iterations\n",
    "                test_file_write_counter -= 1\n",
    "                if not test_file_write_counter:\n",
    "                    test_file_write_counter = 50\n",
    "                    with open(test_file_path, 'w') as f:\n",
    "                        json.dump(dict_test, f, indent=2)\n",
    "\n",
    "            dict_test['test_complete'] = True\n",
    "\n",
    "            with open(test_file_path, 'w') as f:\n",
    "                json.dump(dict_test, f, indent=2)\n",
    "\n",
    "            # Experiment resumption mechanism (i.e., reinitializing the starting index after re-launching the exp)\n",
    "            if test_file_exists and not exp_continuation_started:\n",
    "                script_starting_index = 0\n",
    "        if test_file_exists and not exp_continuation_started:\n",
    "            task_starting_index = 0\n",
    "            exp_continuation_started = True\n",
    "            \n",
    "\n",
    "'''\n",
    "The result of the main function is stored locally in json files\n",
    "'''\n",
    "func_test_path = get_functionality_test_path('ai_code')\n",
    "funct_test_path = os.path.join(func_test_path, 'chatgpt', 'temp_0.8', 'HumanEval_1.json')\n",
    "\n",
    "with open(funct_test_path, 'r') as file:\n",
    "    funct_test_dict = json.load(file)\n",
    "\n",
    "'''\n",
    "Snippet of the obtained functionality test results\n",
    "'''\n",
    "for (key, value) in list(funct_test_dict.items())[1:8]:\n",
    "    print(f'Script: {key}\\n  Result: {value}\\n')"
   ],
   "id": "ac99b97ca4396269",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script: 0.py\n",
      "  Result: {'successful': True}\n",
      "\n",
      "Script: 1.py\n",
      "  Result: {'successful': True}\n",
      "\n",
      "Script: 2.py\n",
      "  Result: {'successful': False, 'error_type': 'AssertionError'}\n",
      "\n",
      "Script: 3.py\n",
      "  Result: {'successful': False, 'error_type': 'AssertionError'}\n",
      "\n",
      "Script: 4.py\n",
      "  Result: {'successful': True}\n",
      "\n",
      "Script: 5.py\n",
      "  Result: {'successful': False, 'error_type': 'AssertionError'}\n",
      "\n",
      "Script: 6.py\n",
      "  Result: {'successful': False, 'error_type': 'NameError', 'error_message': \"name 'List' is not defined. Did you mean\"}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next step is measuring the __metric__ score against __canonical implementation__ of HumanEval and save it in a *csv* file along with the __pass/fail__ label for each script: ",
   "id": "7d4ec879daa62fdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T08:42:23.620882Z",
     "start_time": "2025-05-22T08:42:23.576692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "import signal\n",
    "import pandas as pd\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "stderr = io.StringIO()\n",
    "with contextlib.redirect_stderr(stderr):\n",
    "    # Supress warning about missing installation of a deeplearning framework\n",
    "    import evaluate as ev\n",
    "\n",
    "class Metric(Enum):\n",
    "    bleu = 0\n",
    "    codebleu = 1\n",
    "    rouge = 2\n",
    "    meteor = 3\n",
    "    chrf = 4\n",
    "\n",
    "\n",
    "# noinspection PyUnusedLocal\n",
    "def timeout_handler(signum, frame):\n",
    "    # Custom TimeOut exception used in 'test_functionality()' function\n",
    "    raise TimeoutError('Execution timeout!')\n",
    "\n",
    "\n",
    "# Initializing TimeOut exception\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "\n",
    "def calculate_metric(metric, baseline, generated_script, metric_calc=None):\n",
    "    \"\"\"\n",
    "    Function that measures the LLM-script score of a given metric against the HumanEval implementation\n",
    "\n",
    "    :param metric: integer that represents the desired metric to be used\n",
    "    :param baseline: HumanEval script\n",
    "    :param generated_script: LLM-generated script\n",
    "    :param metric_calc: preloaded metric module\n",
    "    :return: metric score\n",
    "    \"\"\"\n",
    "    metric_name = Metric(metric).name\n",
    "\n",
    "    score = {}\n",
    "\n",
    "    if not generated_script:\n",
    "        if metric != 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return {\"codebleu\": 0.0,\n",
    "                    \"ngram_match_score\": 0.0,\n",
    "                    \"weighted_ngram_match_score\": 0.0,\n",
    "                    \"syntax_match_score\": 0.0,\n",
    "                    \"dataflow_match_score\": 0.0}\n",
    "\n",
    "    if metric == 1:\n",
    "        metric_complete = False\n",
    "        signal.alarm(2)\n",
    "        while not metric_complete:\n",
    "            try:\n",
    "                score = calc_codebleu(predictions=[generated_script], references=[baseline], lang='python')\n",
    "                signal.alarm(0)\n",
    "                metric_complete = True\n",
    "            except TimeoutError:\n",
    "                print('Timeout Error')\n",
    "                signal.alarm(2)\n",
    "\n",
    "    else:\n",
    "        if metric == 2:\n",
    "            results = metric_calc.compute(predictions=[generated_script], references=[baseline], rouge_types=['rougeL'])\n",
    "        else:\n",
    "            results = metric_calc.compute(predictions=[generated_script], references=[baseline])\n",
    "\n",
    "        if metric == 2:\n",
    "            score = results['rougeL']\n",
    "        elif metric == 4:\n",
    "            score = results['score'] / 100\n",
    "        else:\n",
    "            score = results[metric_name]\n",
    "    return score\n",
    "\n",
    "\n",
    "def list_non_hidden_files(directory):\n",
    "    # Function that returns the list of visible files from a given directory\n",
    "    return [f for f in os.listdir(directory) if not f.startswith('.')]\n",
    "\n",
    "\n",
    "def metric_measurement(target_dataset):\n",
    "    \"\"\"\n",
    "    Function that iterates over the LLM-generated scripts and measures the metric score all the studied metrics\n",
    "    :return: writes a csv file with the obtained score as well as pass/fail label for each AI-script\n",
    "    \"\"\"\n",
    "    metric_folder_path = get_metric_results_path(target_dataset)\n",
    "    ai_code_path = get_ai_code_path(target_dataset)\n",
    "    humaneval_baseline_path = os.path.join(code_path, 'humaneval_baseline')\n",
    "    functionality_test_path = get_functionality_test_path(target_dataset)\n",
    "\n",
    "    list_models_and_temps = sorted(os.listdir(ai_code_path))\n",
    "    list_humaneval_scripts = sorted(os.listdir(humaneval_baseline_path))\n",
    "\n",
    "    # Experiment-resumption mechanism\n",
    "    if not os.path.exists(metric_folder_path):\n",
    "        os.mkdir(metric_folder_path)\n",
    "        metric_file_exists = False\n",
    "        script_starting_index = model_and_temp_starting_index = task_starting_index = metric_starting_index = 0\n",
    "\n",
    "    else:\n",
    "        # Obtaining the starting point of exp-resumption\n",
    "        metric_file_exists = True\n",
    "\n",
    "        metric_starting_index = len(os.listdir(metric_folder_path))-1\n",
    "        last_tested_metric = Metric(metric_starting_index).name\n",
    "        metric_folder_name = f'{last_tested_metric}_tasks'\n",
    "        last_tested_metric_path = os.path.join(metric_folder_path, metric_folder_name)\n",
    "        list_tested_tasks = sorted(list_non_hidden_files(last_tested_metric_path), key=custom_sort_key)\n",
    "        task_starting_index = len(list_tested_tasks)-1\n",
    "\n",
    "        task_csv_name = list_tested_tasks[0]\n",
    "        current_task_path = os.path.join(last_tested_metric_path, task_csv_name)\n",
    "        task_metric_df = pd.read_csv(current_task_path)\n",
    "\n",
    "        if 'complete' in list_tested_tasks[0]:\n",
    "            # Skipping to the next task if current task was complete in the previous exp\n",
    "            task_starting_index += 1\n",
    "\n",
    "            if task_starting_index == 163:\n",
    "                metric_starting_index += 1\n",
    "                task_starting_index = 0\n",
    "\n",
    "                if metric_starting_index == 5:\n",
    "                    print('Metric measurement complete')\n",
    "                    exit(0)\n",
    "\n",
    "            model_and_temp_starting_index = 0\n",
    "            script_starting_index = 0\n",
    "\n",
    "        else:\n",
    "            last_row = task_metric_df.tail(1)\n",
    "            last_row_series = last_row.iloc[0]\n",
    "            last_model_and_temp = last_row_series['model&temp']\n",
    "            last_script = last_row_series['script']\n",
    "\n",
    "            task_name = task_csv_name.strip('.csv')\n",
    "            model_and_temp_starting_index = list_models_and_temps.index(last_model_and_temp)\n",
    "            current_model_and_temp_path = os.path.join(ai_code_path, last_model_and_temp, task_name)\n",
    "\n",
    "            list_scripts = sorted(os.listdir(current_model_and_temp_path), key=custom_sort_key)\n",
    "            script_starting_index = list_scripts.index(last_script) + 1\n",
    "\n",
    "            if script_starting_index == len(list_scripts):\n",
    "                model_and_temp_starting_index += 1\n",
    "                script_starting_index = 0\n",
    "\n",
    "    exp_continuation_started = False\n",
    "\n",
    "    for metric_index in range(metric_starting_index, 5):\n",
    "        metric_name = Metric(metric_index).name\n",
    "        print(f'Analyzing metric: {metric_name}')\n",
    "\n",
    "        target_folder_name = f'{metric_name}_tasks'\n",
    "        current_metric_path = os.path.join(metric_folder_path, target_folder_name)\n",
    "        if not os.path.exists(current_metric_path):\n",
    "            os.mkdir(current_metric_path)\n",
    "\n",
    "        # Preloading metric module for all metrics except CodeBLEU\n",
    "        if metric_index != 1:\n",
    "            metric_calc = ev.load(metric_name)\n",
    "        else:\n",
    "            metric_calc = None\n",
    "\n",
    "        for task_index in range(task_starting_index, 163):\n",
    "            if task_index < 145:\n",
    "                task_number = task_index\n",
    "            else:\n",
    "                task_number = task_index + 1\n",
    "\n",
    "            task_name = f'HumanEval_{task_number}'\n",
    "            print(f'Analyzing task: {task_name}')\n",
    "            task_csv_name = task_name + '.csv'\n",
    "            task_csv_path = os.path.join(current_metric_path, task_csv_name)\n",
    "\n",
    "            if os.path.exists(task_csv_path):\n",
    "                task_metric_df = pd.read_csv(task_csv_path)\n",
    "                task_metric = task_metric_df.to_dict('records')\n",
    "            else:\n",
    "                task_metric = []\n",
    "\n",
    "            # Obtaining the HumanEval implementation as a comparison baseline\n",
    "            target_humaneval = list_humaneval_scripts[task_index]\n",
    "            target_humaneval_path = os.path.join(humaneval_baseline_path, target_humaneval)\n",
    "            humaneval_content = open(target_humaneval_path, 'r').read()\n",
    "            humaneval_script = code_cleanup(humaneval_content)\n",
    "\n",
    "            nb_of_models_and_temps = len(list_models_and_temps)\n",
    "\n",
    "            for model_and_temp_index in range(model_and_temp_starting_index, nb_of_models_and_temps):\n",
    "                target_model_and_temp = list_models_and_temps[model_and_temp_index]\n",
    "                print(f'Analyzing model and temp: {target_model_and_temp}')\n",
    "\n",
    "                target_model_and_temp_path = os.path.join(ai_code_path, target_model_and_temp)\n",
    "\n",
    "                target_task_path = os.path.join(target_model_and_temp_path, task_name)\n",
    "                task_scripts = sorted(os.listdir(target_task_path), key=custom_sort_key)\n",
    "\n",
    "                model_name = target_model_and_temp.split('_temp')[0]\n",
    "                model_temp = target_model_and_temp[-8:]\n",
    "\n",
    "                # Loading the functionality-test results for the current model/temp/task (used for the pass/fail label)\n",
    "                target_functionality_test = os.path.join(functionality_test_path, model_name, model_temp,\n",
    "                                                         f'{task_name}.json')\n",
    "                with open(target_functionality_test, 'r') as f:\n",
    "                    funct_test_results = json.load(f)\n",
    "\n",
    "                file_write_counter = 100\n",
    "                for script_index in range(script_starting_index, len(task_scripts)):\n",
    "                    # Extracting and cleaning the LLM-generated script\n",
    "                    script_name = f'{script_index}.py'\n",
    "                    target_script_path = os.path.join(target_model_and_temp_path, task_name, script_name)\n",
    "                    script_content = open(target_script_path).read()\n",
    "                    cleaned_script = code_cleanup(script_content)\n",
    "\n",
    "                    script_test_pass = funct_test_results[script_name]['successful']\n",
    "\n",
    "                    # Measuring the metric score of the current script\n",
    "                    score = calculate_metric(metric_index, humaneval_script, cleaned_script, metric_calc)\n",
    "                    dict_entry = {'model&temp': target_model_and_temp,\n",
    "                                  'script': script_name,\n",
    "                                  'pass': script_test_pass}\n",
    "\n",
    "                    if metric_index != 1:\n",
    "                        dict_entry.update({'score': score})\n",
    "\n",
    "                    else:\n",
    "                        entry_addition = {'codebleu': score['codebleu'],\n",
    "                                          'ngram_match_score': score['ngram_match_score'],\n",
    "                                          'weighted_ngram_match_score': score['weighted_ngram_match_score'],\n",
    "                                          'syntax_match_score': score['syntax_match_score'],\n",
    "                                          'dataflow_match_score': score['dataflow_match_score']}\n",
    "                        dict_entry.update(entry_addition)\n",
    "                    task_metric.append(dict_entry)\n",
    "\n",
    "                    # Writing the results in a csv file every 100 iterations\n",
    "                    file_write_counter -= 1\n",
    "                    if not file_write_counter or script_index == 199:\n",
    "                        task_metric_df = pd.DataFrame.from_records(task_metric)\n",
    "                        task_metric_df.to_csv(task_csv_path, index=False)\n",
    "                        file_write_counter = 100\n",
    "\n",
    "                # Experiment resumption mechanism (i.e., reinitializing the starting index after re-launching the exp)\n",
    "                if metric_file_exists and not exp_continuation_started:\n",
    "                    script_starting_index = 0\n",
    "\n",
    "            if metric_file_exists and not exp_continuation_started:\n",
    "                model_and_temp_starting_index = 0\n",
    "\n",
    "            # Marking the resulting csv file as complete\n",
    "            os.remove(task_csv_path)\n",
    "            task_csv_name = f'{task_name}-complete.csv'\n",
    "            task_csv_path = os.path.join(metric_folder_path, metric_name, task_csv_name)\n",
    "            task_metric_df = pd.DataFrame.from_records(task_metric)\n",
    "            task_metric_df.to_csv(task_csv_path, index=False)\n",
    "\n",
    "        if metric_file_exists and not exp_continuation_started:\n",
    "            task_starting_index = 0\n",
    "            exp_continuation_started = True\n",
    "\n",
    "\n",
    "'''\n",
    "The metric score is saved locally in a .csv files\n",
    "'''\n",
    "metric_results_path = get_metric_results_path('ai_code')\n",
    "example_metric_score_path = os.path.join(metric_results_path, 'bleu_tasks', 'HumanEval_1-complete.csv')\n",
    "metric_results = pd.read_csv(example_metric_score_path)\n",
    "\n",
    "print(metric_results.head(10))"
   ],
   "id": "995c9dd8a8993c79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         model&temp script   pass     score\n",
      "0  chatgpt_temp_0.0   0.py   True  0.166768\n",
      "1  chatgpt_temp_0.8   0.py   True  0.209111\n",
      "2  chatgpt_temp_0.8   1.py   True  0.246753\n",
      "3  chatgpt_temp_0.8   2.py  False  0.188520\n",
      "4  chatgpt_temp_0.8   3.py  False  0.277207\n",
      "5  chatgpt_temp_0.8   4.py   True  0.259093\n",
      "6  chatgpt_temp_0.8   5.py  False  0.227023\n",
      "7  chatgpt_temp_0.8   6.py  False  0.163480\n",
      "8  chatgpt_temp_0.8   7.py   True  0.223758\n",
      "9  chatgpt_temp_0.8   8.py  False  0.211294\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once all the __metric__ scores are measured, we can start applying *Logistic Regression* training based on the __metric score__ and the __pass/fail__ label in order to establish if a correlation exists between the two. In order to do so, we first run __100 iterations__ of machine learning and prediction per __metric__ (which gives us the scores for: *precision*, *recall*, *f1* and *accuracy*) and then we measure the *average* and *variance* values of obtained evaluation scores.\n",
   "id": "14aebf6511499655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T08:42:31.969143Z",
     "start_time": "2025-05-22T08:42:26.858523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def run_logistic_regression(target_dataset, nb_iterations = 100):\n",
    "    \"\"\"\n",
    "    Function that runs logistic regression over the LLM-generated script results (i.e., establish if a correlation\n",
    "    exists between metric score and pass/fail label)\n",
    "    :return: a json file with scores for precision, recall, f1, accuracy\n",
    "    \"\"\"\n",
    "    metric_score_path = get_metric_results_path(target_dataset)\n",
    "\n",
    "    logreg_results_path = get_logreg_results_path(target_dataset)\n",
    "    logreg_iterations_path = os.path.join(logreg_results_path, 'iterations')\n",
    "\n",
    "    os.makedirs(logreg_iterations_path, exist_ok=True)\n",
    "\n",
    "    for item in sorted(os.listdir(metric_score_path)):\n",
    "        logreg_dict = {'decision_boundary': 0.5}\n",
    "        logreg_test_pred_dict = {}\n",
    "        # Iterate over csv files with the metric score and the pass/fail label of LLM-generated scripts\n",
    "        if '.csv' in item:\n",
    "            metric_name = item\n",
    "\n",
    "            metric_path = os.path.join(metric_score_path, metric_name)\n",
    "            metric_df = pd.read_csv(metric_path)\n",
    "\n",
    "            metric_name = metric_name.split('.')[0]\n",
    "            logreg_file_name = f'{metric_name}_logreg_iterations.json'\n",
    "            test_pred_file_name = f'{metric_name}_logreg_test_pred.json'\n",
    "            logreg_file_path = os.path.join(logreg_iterations_path, logreg_file_name)\n",
    "            test_pred_file_path = os.path.join(logreg_iterations_path, test_pred_file_name)\n",
    "\n",
    "            print(f'Analyzing metric: {metric_name}')\n",
    "\n",
    "            if 'codebleu' in metric_name:\n",
    "                x = metric_df[['codebleu']]\n",
    "            else:\n",
    "                x = metric_df[['score']]\n",
    "            y = metric_df['pass']\n",
    "\n",
    "            # Run 100 iterations of logistic regression with different split of train/test datasets\n",
    "            for i in range(nb_iterations):\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    print(f'Iteration {i+1}')\n",
    "\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "                logreg = LogisticRegression(random_state=16)\n",
    "                logreg.fit(x_train, y_train)\n",
    "                y_pred = logreg.predict(x_test)\n",
    "\n",
    "                # Saving the classification evaluation results (precision, recall, f1, accuracy)\n",
    "                logreg_results = classification_report(y_test, y_pred, target_names=['fail', 'pass'], output_dict=True)\n",
    "                logreg_dict[f'iter_{i+1}'] = logreg_results\n",
    "\n",
    "                # Saving the ground truth labels (pass/fail) and the predict labels\n",
    "                logreg_test_pred_dict[f'iter_{i + 1}'] = {}\n",
    "                logreg_test_pred_dict[f'iter_{i+1}']['y_test'] = y_test.tolist()\n",
    "                logreg_test_pred_dict[f'iter_{i+1}']['y_pred'] = y_pred.tolist()\n",
    "\n",
    "            with open(logreg_file_path, 'w') as f:\n",
    "                json.dump(logreg_dict, f, indent=2)\n",
    "            with open(test_pred_file_path, 'w') as f:\n",
    "                json.dump(logreg_test_pred_dict, f)\n",
    "\n",
    "\n",
    "def divide_by(input_dict, divide = 100):\n",
    "    # Function that divides the obtained scores for the average calculation\n",
    "    input_dict['pass']['precision'] /= divide\n",
    "    input_dict['pass']['recall'] /= divide\n",
    "    input_dict['pass']['f1-score'] /= divide\n",
    "\n",
    "    input_dict['fail']['precision'] /= divide\n",
    "    input_dict['fail']['recall'] /= divide\n",
    "    input_dict['fail']['f1-score'] /= divide\n",
    "\n",
    "    input_dict['accuracy'] /= divide\n",
    "\n",
    "    input_dict['macro avg']['precision'] /= divide\n",
    "    input_dict['macro avg']['recall'] /= divide\n",
    "    input_dict['macro avg']['f1-score'] /= divide\n",
    "\n",
    "    input_dict['weighted avg']['precision'] /= divide\n",
    "    input_dict['weighted avg']['recall'] /= divide\n",
    "    input_dict['weighted avg']['f1-score'] /= divide\n",
    "\n",
    "\n",
    "def logreg_average_variance(target_dataset, nb_iterations = 100):\n",
    "    # Function that measures the average and variance values of the 100 logreg-iteration results\n",
    "    logreg_results_path = get_logreg_results_path(target_dataset)\n",
    "    logreg_iterations_path = os.path.join(logreg_results_path, 'iterations')\n",
    "\n",
    "    metrics_template = {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}\n",
    "    logreg_template = {'pass': copy.deepcopy(metrics_template),\n",
    "                       'fail': copy.deepcopy(metrics_template),\n",
    "                       'accuracy': 0.0,\n",
    "                       'macro avg': copy.deepcopy(metrics_template),\n",
    "                       'weighted avg': copy.deepcopy(metrics_template)}\n",
    "\n",
    "    for file_name in sorted(os.listdir(logreg_iterations_path)):\n",
    "        if 'iterations' in file_name:\n",
    "            current_file_path = os.path.join(logreg_iterations_path, file_name)\n",
    "            with open(current_file_path, 'r') as f:\n",
    "                logreg_dict = json.load(f)\n",
    "\n",
    "            metric_name = file_name.split('_')[0]\n",
    "            logreg_file_name = f'{metric_name}_logreg_avg-var.json'\n",
    "            avg_var_folder_path = logreg_iterations_path.rpartition('/')[0]\n",
    "            logreg_file_path = os.path.join(avg_var_folder_path, logreg_file_name)\n",
    "\n",
    "            logreg_avg_var = {\n",
    "                \"decision_boundary\": 0.5,\n",
    "                'average': copy.deepcopy(logreg_template),\n",
    "                'variance': copy.deepcopy(logreg_template)}\n",
    "\n",
    "            for iteration in list(logreg_dict.keys())[1:]:\n",
    "                logreg_avg_var['average']['pass']['precision'] += logreg_dict[iteration]['pass']['precision']\n",
    "                logreg_avg_var['average']['pass']['recall'] += logreg_dict[iteration]['pass']['recall']\n",
    "                logreg_avg_var['average']['pass']['f1-score'] += logreg_dict[iteration]['pass']['f1-score']\n",
    "                logreg_avg_var['average']['pass']['support'] += logreg_dict[iteration]['pass']['support']\n",
    "\n",
    "                logreg_avg_var['average']['fail']['precision'] += logreg_dict[iteration]['fail']['precision']\n",
    "                logreg_avg_var['average']['fail']['recall'] += logreg_dict[iteration]['fail']['recall']\n",
    "                logreg_avg_var['average']['fail']['f1-score'] += logreg_dict[iteration]['fail']['f1-score']\n",
    "                logreg_avg_var['average']['fail']['support'] += logreg_dict[iteration]['fail']['support']\n",
    "\n",
    "                logreg_avg_var['average']['accuracy'] += logreg_dict[iteration]['accuracy']\n",
    "\n",
    "                logreg_avg_var['average']['macro avg']['precision'] += logreg_dict[iteration]['macro avg']['precision']\n",
    "                logreg_avg_var['average']['macro avg']['recall'] += logreg_dict[iteration]['macro avg']['recall']\n",
    "                logreg_avg_var['average']['macro avg']['f1-score'] += logreg_dict[iteration]['macro avg']['f1-score']\n",
    "                logreg_avg_var['average']['macro avg']['support'] += logreg_dict[iteration]['macro avg']['support']\n",
    "\n",
    "                logreg_avg_var['average']['weighted avg']['precision'] += (\n",
    "                    logreg_dict)[iteration]['weighted avg']['precision']\n",
    "                logreg_avg_var['average']['weighted avg']['recall'] += (\n",
    "                    logreg_dict)[iteration]['weighted avg']['recall']\n",
    "                logreg_avg_var['average']['weighted avg']['f1-score'] += (\n",
    "                    logreg_dict)[iteration]['weighted avg']['f1-score']\n",
    "                logreg_avg_var['average']['weighted avg']['support'] += (\n",
    "                    logreg_dict)[iteration]['weighted avg']['support']\n",
    "\n",
    "            divide_by(logreg_avg_var['average'], nb_iterations)\n",
    "\n",
    "            for iteration in list(logreg_dict.keys())[1:]:\n",
    "                logreg_avg_var['variance']['pass']['precision'] += abs(logreg_avg_var['average']['pass']['precision'] -\n",
    "                                                                       logreg_dict[iteration]['pass']['precision'])\n",
    "                logreg_avg_var['variance']['pass']['recall'] += abs(logreg_avg_var['average']['pass']['recall'] -\n",
    "                                                                    logreg_dict[iteration]['pass']['recall'])\n",
    "                logreg_avg_var['variance']['pass']['f1-score'] += abs(logreg_avg_var['average']['pass']['f1-score'] -\n",
    "                                                                      logreg_dict[iteration]['pass']['f1-score'])\n",
    "                logreg_avg_var['variance']['pass']['support'] = logreg_avg_var['average']['pass']['support']\n",
    "\n",
    "                logreg_avg_var['variance']['fail']['precision'] += abs(\n",
    "                    logreg_avg_var['average']['fail']['precision'] -\n",
    "                    logreg_dict[iteration]['fail']['precision'])\n",
    "                logreg_avg_var['variance']['fail']['recall'] += abs(\n",
    "                    logreg_avg_var['average']['fail']['recall'] -\n",
    "                    logreg_dict[iteration]['fail']['recall'])\n",
    "                logreg_avg_var['variance']['fail']['f1-score'] += abs(\n",
    "                    logreg_avg_var['average']['fail']['f1-score'] -\n",
    "                    logreg_dict[iteration]['fail']['f1-score'])\n",
    "                logreg_avg_var['variance']['fail']['support'] = logreg_avg_var['average']['fail']['support']\n",
    "\n",
    "                logreg_avg_var['variance']['accuracy'] += abs(logreg_avg_var ['average']['accuracy'] -\n",
    "                                                              logreg_dict[iteration]['accuracy'])\n",
    "\n",
    "                logreg_avg_var['variance']['macro avg']['precision'] += abs(\n",
    "                    logreg_avg_var['average']['macro avg']['precision'] -\n",
    "                    logreg_dict[iteration]['macro avg']['precision'])\n",
    "                logreg_avg_var['variance']['macro avg']['recall'] += abs(\n",
    "                    logreg_avg_var['average']['macro avg']['recall'] -\n",
    "                    logreg_dict[iteration]['macro avg']['recall'])\n",
    "                logreg_avg_var['variance']['macro avg']['f1-score'] += abs(\n",
    "                    logreg_avg_var['average']['macro avg']['f1-score'] -\n",
    "                    logreg_dict[iteration]['macro avg']['f1-score'])\n",
    "                logreg_avg_var['variance']['macro avg']['support'] = logreg_avg_var['average']['macro avg']['support']\n",
    "\n",
    "                logreg_avg_var['variance']['weighted avg']['precision'] += abs(\n",
    "                    logreg_avg_var['average']['weighted avg']['precision'] -\n",
    "                    logreg_dict[iteration]['weighted avg']['precision'])\n",
    "                logreg_avg_var['variance']['weighted avg']['recall'] += abs(\n",
    "                    logreg_avg_var['average']['weighted avg']['recall'] -\n",
    "                    logreg_dict[iteration]['weighted avg']['recall'])\n",
    "                logreg_avg_var['variance']['weighted avg']['f1-score'] += abs(\n",
    "                    logreg_avg_var['average']['weighted avg']['f1-score'] -\n",
    "                    logreg_dict[iteration]['weighted avg']['f1-score'])\n",
    "                logreg_avg_var['variance']['weighted avg']['support'] = (\n",
    "                    logreg_avg_var)['average']['weighted avg']['support']\n",
    "\n",
    "            with open(logreg_file_path, 'w') as f:\n",
    "                json.dump(logreg_avg_var, f, indent=2)\n",
    "\n",
    "\n",
    "def metric_name_to_title(metric_name):\n",
    "    # Function that returns the name of a metric used in the confusion matrix representation\n",
    "    title = ''\n",
    "    match metric_name:\n",
    "        case 'bleu':\n",
    "            title = 'BLEU'\n",
    "        case 'codebleu':\n",
    "            title = 'CodeBLEU'\n",
    "        case 'rouge':\n",
    "            title = 'ROUGE'\n",
    "        case 'meteor':\n",
    "            title = 'METEOR'\n",
    "        case 'chrf':\n",
    "            title = 'ChrF'\n",
    "    return title\n",
    "\n",
    "\n",
    "def format_logreg_results(logreg_dict):\n",
    "    row_format = '{:<12} {:>10.2f} {:>10.2f} {:>10.2f} {:>10}'\n",
    "    row_format_accuracy = '{:<33}  {:>10.2f} {:>10}'\n",
    "\n",
    "    formated_rows = []\n",
    "\n",
    "    for label in ['pass', 'fail']:\n",
    "        row = row_format.format(\n",
    "            label,\n",
    "            logreg_dict[label]['precision'],\n",
    "            logreg_dict[label]['recall'],\n",
    "            logreg_dict[label]['f1-score'],\n",
    "            int(logreg_dict[label]['support'])\n",
    "        )\n",
    "        formated_rows.append(row)\n",
    "\n",
    "    accuracy_row = row_format_accuracy.format(\n",
    "        'accuracy',\n",
    "        logreg_dict['accuracy'],\n",
    "        int(logreg_dict['pass']['support'] + logreg_dict['fail']['support'])\n",
    "    )\n",
    "\n",
    "    for avg_label in ['macro avg', 'weighted avg']:\n",
    "        avg_row = row_format.format(\n",
    "            avg_label,\n",
    "            logreg_dict[avg_label]['precision'],\n",
    "            logreg_dict[avg_label]['recall'],\n",
    "            logreg_dict[avg_label]['f1-score'],\n",
    "            int(logreg_dict[avg_label]['support'])\n",
    "        )\n",
    "        formated_rows.append(avg_row)\n",
    "    formated_rows.insert(2, f'\\n{accuracy_row}')\n",
    "    return '\\n'.join(formated_rows)\n",
    "\n",
    "\n",
    "original_dataset = 'ai_code'\n",
    "number_iterations = 50\n",
    "run_logistic_regression(original_dataset, number_iterations)\n",
    "logreg_average_variance(original_dataset, number_iterations)\n"
   ],
   "id": "cc1d423c864e36b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing metric: bleu\n",
      "Iteration 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 254\u001B[39m\n\u001B[32m    252\u001B[39m original_dataset = \u001B[33m'\u001B[39m\u001B[33mai_code\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    253\u001B[39m number_iterations = \u001B[32m50\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m254\u001B[39m \u001B[43mrun_logistic_regression\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_iterations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    255\u001B[39m logreg_average_variance(original_dataset, number_iterations)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 58\u001B[39m, in \u001B[36mrun_logistic_regression\u001B[39m\u001B[34m(target_dataset, nb_iterations)\u001B[39m\n\u001B[32m     55\u001B[39m y_pred = logreg.predict(x_test)\n\u001B[32m     57\u001B[39m \u001B[38;5;66;03m# Saving the classification evaluation results (precision, recall, f1, accuracy)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m logreg_results = \u001B[43mclassification_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_names\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfail\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpass\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     59\u001B[39m logreg_dict[\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33miter_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m] = logreg_results\n\u001B[32m     61\u001B[39m \u001B[38;5;66;03m# Saving the ground truth labels (pass/fail) and the predict labels\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    212\u001B[39m         skip_parameter_validation=(\n\u001B[32m    213\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    214\u001B[39m         )\n\u001B[32m    215\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    218\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    219\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    222\u001B[39m     msg = re.sub(\n\u001B[32m    223\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    224\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    225\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    226\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2703\u001B[39m, in \u001B[36mclassification_report\u001B[39m\u001B[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001B[39m\n\u001B[32m   2701\u001B[39m headers = [\u001B[33m\"\u001B[39m\u001B[33mprecision\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mrecall\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mf1-score\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33msupport\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   2702\u001B[39m \u001B[38;5;66;03m# compute per-class results without averaging\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2703\u001B[39m p, r, f1, s = \u001B[43mprecision_recall_fscore_support\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2704\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2705\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2706\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2707\u001B[39m \u001B[43m    \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   2708\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2709\u001B[39m \u001B[43m    \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[43m=\u001B[49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2710\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2711\u001B[39m rows = \u001B[38;5;28mzip\u001B[39m(target_names, p, r, f1, s)\n\u001B[32m   2713\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m y_type.startswith(\u001B[33m\"\u001B[39m\u001B[33mmultilabel\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:189\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m func_sig = signature(func)\n\u001B[32m    193\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1834\u001B[39m, in \u001B[36mprecision_recall_fscore_support\u001B[39m\u001B[34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[39m\n\u001B[32m   1832\u001B[39m \u001B[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001B[39;00m\n\u001B[32m   1833\u001B[39m samplewise = average == \u001B[33m\"\u001B[39m\u001B[33msamples\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1834\u001B[39m MCM = \u001B[43mmultilabel_confusion_matrix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1835\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1836\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1837\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1838\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1839\u001B[39m \u001B[43m    \u001B[49m\u001B[43msamplewise\u001B[49m\u001B[43m=\u001B[49m\u001B[43msamplewise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1840\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1841\u001B[39m tp_sum = MCM[:, \u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m]\n\u001B[32m   1842\u001B[39m pred_sum = tp_sum + MCM[:, \u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:189\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    191\u001B[39m func_sig = signature(func)\n\u001B[32m    193\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/metrics/_classification.py:562\u001B[39m, in \u001B[36mmultilabel_confusion_matrix\u001B[39m\u001B[34m(y_true, y_pred, sample_weight, labels, samplewise)\u001B[39m\n\u001B[32m    560\u001B[39m le.fit(labels)\n\u001B[32m    561\u001B[39m y_true = le.transform(y_true)\n\u001B[32m--> \u001B[39m\u001B[32m562\u001B[39m y_pred = \u001B[43mle\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    563\u001B[39m sorted_labels = le.classes_\n\u001B[32m    565\u001B[39m \u001B[38;5;66;03m# labels are now from 0 to len(labels) - 1 -> use bincount\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:134\u001B[39m, in \u001B[36mLabelEncoder.transform\u001B[39m\u001B[34m(self, y)\u001B[39m\n\u001B[32m    131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _num_samples(y) == \u001B[32m0\u001B[39m:\n\u001B[32m    132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m xp.asarray([])\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muniques\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclasses_\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/utils/_encode.py:235\u001B[39m, in \u001B[36m_encode\u001B[39m\u001B[34m(values, uniques, check_unknown)\u001B[39m\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m xp.isdtype(values.dtype, \u001B[33m\"\u001B[39m\u001B[33mnumeric\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    234\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_map_to_integer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muniques\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    236\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    237\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33my contains previously unseen labels: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/utils/_encode.py:174\u001B[39m, in \u001B[36m_map_to_integer\u001B[39m\u001B[34m(values, uniques)\u001B[39m\n\u001B[32m    172\u001B[39m xp, _ = get_namespace(values, uniques)\n\u001B[32m    173\u001B[39m table = _nandict({val: i \u001B[38;5;28;01mfor\u001B[39;00m i, val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(uniques)})\n\u001B[32m--> \u001B[39m\u001B[32m174\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mxp\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m[\u001B[49m\u001B[43mv\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/sklearn/utils/_array_api.py:407\u001B[39m, in \u001B[36m_NumPyAPIWrapper.asarray\u001B[39m\u001B[34m(self, x, dtype, device, copy)\u001B[39m\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy.array(x, copy=\u001B[38;5;28;01mTrue\u001B[39;00m, dtype=dtype)\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy.asarray(x, dtype=dtype)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation metrics explanation\n",
    "\n",
    "![image info](./images/metric_formulas/precision.png)\n",
    "\n",
    "&NewLine;\n",
    "Among predicted labels, how many of them are correct.\n",
    "The above formula is for predictions of 'Positive' (or '*Pass*' in our case).\n",
    "However, the same formula can also apply to the '*Fail*' prediction: TN / (TN + FN)\n",
    "\n",
    "___\n",
    "![image info](./images/metric_formulas/recall.png)\n",
    "\n",
    "&NewLine;\n",
    "Among the existing labels, how many were correctly predicted (e.g., how many actual *Pass* scripts were predicted as such)\n",
    "___\n",
    "![image info](./images/metric_formulas/f1-score.png)\n",
    "\n",
    "&NewLine;\n",
    "F1-score measures a balance between *Precision* and *Recall*. This metric indicates the model's per-label performance.\n",
    "___\n",
    "![image info](./images/metric_formulas/accuracy.png)\n",
    "\n",
    "&NewLine;\n",
    "Overall number of correct predictions. It is important to note that this metric is only relevant when the dataset is balanced (i.e., equivalent number of *Pass/Fail* labels) and when correctly predicting both labels is equally important, which is not always the case.\n",
    "___\n",
    "&NewLine;\n",
    "&NewLine;\n",
    "\n",
    "__Support__ = number of ground-truth *Pass/Fail* labels\n",
    "___\n",
    "\n",
    "&NewLine;\n",
    "&NewLine;\n",
    "#### Macro VS Weighted average:\n",
    "- Macro is the standard average value of the precision, recall, and F1-score, without taking into account the number of samples in each label. It treats all labels equally.\n",
    "- Weighted is the average of the precision, recall, and F1-score which is weighted by the number of samples in each label. It accounts for the imbalance in the number of samples per label.\n",
    "\n",
    "The following are the results of the *LinearRegression* training/testing done previously in the notebook."
   ],
   "id": "57fc34b2ac80b91a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T08:42:35.226616Z",
     "start_time": "2025-05-22T08:42:35.104782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_logreg_avg_var(target_dataset, metric):\n",
    "    metric_name = Metric(metric).name\n",
    "    metric_title = metric_name_to_title(metric_name)\n",
    "\n",
    "    logreg_results_path = get_logreg_results_path(target_dataset)\n",
    "    logreg_avg_var_path = os.path.join(logreg_results_path, f'{metric_name}_logreg_avg-var.json')\n",
    "\n",
    "    with open(logreg_avg_var_path, 'r') as f:\n",
    "        logreg_dict = json.load(f)\n",
    "\n",
    "    first_section = True\n",
    "\n",
    "    print(f'Logistic Regression results for \\\"{metric_title}\\\" metric (average and variance):\\n')\n",
    "    for dict_key in list(logreg_dict.keys())[1:]:\n",
    "        print(f'{dict_key}:')\n",
    "        print(f\"{' ':<12} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\")\n",
    "        print(format_logreg_results(logreg_dict[dict_key]))\n",
    "        if first_section:\n",
    "            print('-' * 60)\n",
    "            first_section = False\n",
    "\n",
    "\n",
    "# Textual metrics\n",
    "bleu = 0\n",
    "codebleu = 1\n",
    "rouge = 2\n",
    "meteor = 3\n",
    "chrf = 4\n",
    "\n",
    "display_logreg_avg_var(original_dataset, metric=bleu)"
   ],
   "id": "3bc8211564a43279",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/zara/textual-metrics/metric_exp_data/exp_results/ai_code/metrics_logreg/bleu_logreg_avg-var.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 30\u001B[39m\n\u001B[32m     27\u001B[39m meteor = \u001B[32m3\u001B[39m\n\u001B[32m     28\u001B[39m chrf = \u001B[32m4\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[43mdisplay_logreg_avg_var\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbleu\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mdisplay_logreg_avg_var\u001B[39m\u001B[34m(target_dataset, metric)\u001B[39m\n\u001B[32m      5\u001B[39m logreg_results_path = get_logreg_results_path(target_dataset)\n\u001B[32m      6\u001B[39m logreg_avg_var_path = os.path.join(logreg_results_path, \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_logreg_avg-var.json\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlogreg_avg_var_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m      9\u001B[39m     logreg_dict = json.load(f)\n\u001B[32m     11\u001B[39m first_section = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/metric-proxy/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    321\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '/home/zara/textual-metrics/metric_exp_data/exp_results/ai_code/metrics_logreg/bleu_logreg_avg-var.json'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(original_dataset, metric=codebleu)",
   "id": "f021bc8f9e641cbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(original_dataset, metric=rouge)",
   "id": "bf8a5ff3e212fcfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(original_dataset, metric=meteor)",
   "id": "83f3f8867e94bf52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(original_dataset, metric=chrf)",
   "id": "10dce7610ad171d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analysis of predictive models' efficiency\n",
    "The results obtained above point to the fact that, no matter the chosen *textual-metric*, the predictive models struggle at correctly predicting the actual labels:\n",
    "\n",
    "At a first glance, the models seem to correctly predict the *Fail* label, with precision, recall and f1-scores values ranging between 80%-97%. However, this is potentially due to the highly __unbalanced__ nature of the dataset: this can be seen in the difference of the *support* value for both labels. Roughly 77% of AI-generated scripts do not pass the tests, either due to undesired behavior or execution errors.\n",
    "\n",
    "The predictive models' struggle becomes mo obvious when observing the results for predicting the *Pass* label:\n",
    "- Bad *Precision* ~55%-65%: meaning that almost half of the *Pass* predictions are actually *Fail*\n",
    "- Even worse *Recall* ~15%-25%: more than 3/4 of *Pass* scripts are labeled as *Fail*\n",
    "- High variance: the trained models have highly varying degrees of correctly predicting the *Pass* label\n",
    "\n",
    "___\n",
    "## Confusion Matrix\n",
    "A more visual way of representing the results of the predictive models is the *Confusion Matrix*, which displays the ratio of the correct/wrong predictions."
   ],
   "id": "4b9660002be3f1cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "def generate_confusion_matrix(dataset_name, nb_iterations=100, font_size=14):\n",
    "    # Generate the confusion matrix based on the ground truth and predicted labels of pass/fail\n",
    "    logreg_res_folder_path = os.path.join(results_path, dataset_name, 'metrics_logreg', 'iterations')\n",
    "    matrix_folder_path = f'./images/confusion_matrix/'\n",
    "\n",
    "    os.makedirs(matrix_folder_path, exist_ok=True)\n",
    "\n",
    "    # Generate the confusion matrix per code-quality metric\n",
    "    for logreg_result in sorted(os.listdir(logreg_res_folder_path)):\n",
    "        if 'test_pred' in logreg_result:\n",
    "            metric_name = logreg_result.split('_')[0]\n",
    "            matrix_title = metric_name_to_title(metric_name)\n",
    "\n",
    "            print(f'Generating confusion matrix for metric: {metric_name}')\n",
    "\n",
    "            if 'v2' in logreg_result:\n",
    "                file_name = f'{metric_name}_v2.png'\n",
    "            else:\n",
    "                file_name = f'{metric_name}.png'\n",
    "\n",
    "            image_file_path = os.path.join(matrix_folder_path, file_name)\n",
    "\n",
    "            test_pred_file_path = os.path.join(logreg_res_folder_path, logreg_result)\n",
    "            with open(test_pred_file_path, 'r') as f:\n",
    "                test_pred_dict = json.load(f)\n",
    "\n",
    "            # Cumulate the 100 iteration confusion matrices\n",
    "            cumulative_confusion_matrix = np.zeros((2, 2), dtype=int)\n",
    "\n",
    "            for iteration in range(nb_iterations):\n",
    "                current_iteration = f'iter_{iteration+1}'\n",
    "                y_test = test_pred_dict[current_iteration]['y_test']\n",
    "                y_pred = test_pred_dict[current_iteration]['y_pred']\n",
    "\n",
    "                current_confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "                cumulative_confusion_matrix += current_confusion_matrix\n",
    "\n",
    "            average_confusion_matrix = cumulative_confusion_matrix / nb_iterations\n",
    "\n",
    "            total_predictions = cumulative_confusion_matrix.sum()\n",
    "\n",
    "            # Convert each element to percentage\n",
    "            percentage_confusion_matrix = (average_confusion_matrix / total_predictions) * nb_iterations * 100\n",
    "\n",
    "            class_names = ['fail', 'pass']\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            tick_marks = np.arange(len(class_names))\n",
    "            plt.xticks(tick_marks, class_names, fontsize=font_size)\n",
    "            plt.yticks(tick_marks, class_names, fontsize=font_size)\n",
    "\n",
    "            # Create the heatmap\n",
    "            sns.heatmap(pd.DataFrame(percentage_confusion_matrix), annot=True, cmap='YlGnBu', fmt='.2f',\n",
    "                        xticklabels=class_names, yticklabels=class_names, ax=ax,\n",
    "                        annot_kws={\"size\": font_size})\n",
    "            ax.xaxis.set_label_position('top')\n",
    "            plt.title(f'{matrix_title}', y=1.05, fontsize=font_size + 2)  # Adjust the title position\n",
    "            plt.ylabel('Actual label', fontsize=font_size)\n",
    "            plt.xlabel('Predicted label', fontsize=font_size)\n",
    "\n",
    "            plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "\n",
    "            # Save and display the figure\n",
    "            fig.savefig(image_file_path, dpi=96)\n",
    "            plt.close(fig)\n",
    "\n",
    "\"\"\"\n",
    "Note: In order to visualize the generated confusion matrices, you have to start editing the next Markdown section and simply exit in order for the IDE to load the images.\n",
    "\"\"\"\n",
    "generate_confusion_matrix(original_dataset, nb_iterations=number_iterations)"
   ],
   "id": "f04e343bb627f5db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![image info](./images/confusion_matrix/bleu.png) ![image info](./images/confusion_matrix/codebleu.png)\n",
    "![image info](./images/confusion_matrix/rouge.png) ![image info](./images/confusion_matrix/meteor.png)\n",
    "![image info](./images/confusion_matrix/chrf.png)"
   ],
   "id": "516c5ccca64ca0bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix analysis\n",
    "\n",
    "This way of representing the prediction results gives us a more global overview of the trained models' efficiency, which points to an overwhelming under-performance. The first noticeable aspect is the quantity of True Negatives (~75%), which is consistent with the previous statement about the unbalanced nature of the dataset. The second noticeable aspect is the quantity of False Negatives, meaning scripts that __pass__ the tests but are predicted as *Fail*. The percentage of False Negatives is roughly 15%-18%, which is on average 3 to 4 times more than True Positives. The last note-worthy detail is the ratio between True Positives and False Positives, which points to the fact that 1/3 of scripts predicted as *Pass* actually __fail__ the tests.\n",
    "\n",
    "\n",
    "Given these results, it is safe to assume that the training of Logistic Regression models failed. This could be due to the highly unbalanced nature of the dataset -- 77% of failing scripts -- which can lead to a \"negative\" training of predictive models. However, it is possible that the nature of dataset is not the only cause of the underwhelming results. Among the metrics that qualify the predictive efficiency of trained models, we can notice that the *variance* for the *Pass* predictions is very high, meaning that the models' capability of predicting the *Pass* label is highly unstable. This could potentially point to the fact that there is no definite correlation between the textual-metric score and the script's test result.\n",
    "\n",
    "___\n",
    "## Textual-metrics and programming languages\n",
    "\n",
    "Textual-metrics were initially created for qualifying machine-generated natural language (e.g., machine translation, automatic summarization). Although there are some similarities between natural and programming languages, there are some key differences that could make textual-metrics unsuitable for *code qualification*. While both natural and programming languages have a predefined structure and syntactical rules, the latter is usually stricter and requires a more robust formulation; for example, missing parenthesis or a semicolon will make a script unable to compile, while in natural text it can simply change the meaning of a sentence without completely breaking it. Such minute details would have very little impact on the textual difference between a reference and a prediction, which can lead to textual-metrics being too __lax__ when qualifying code.\n",
    "\n",
    "*Example of an AI-generated script that contains natural text outside a comment which leads to a compilation error*\n",
    "\n",
    "![image info](./images/ai_code_errors/python_error.PNG)\n",
    "\n",
    "On the other hand, in contrast with natural text, programming languages give more freedom in the naming of variables and functions. While human developers choose meaningful and coherent names for variables and functions, the parser will accept any name as long as it respects a limited amount of rules (e.g., no white spaces or special characters). Furthermore, the names of variables and functions have virtually no impact on the execution logic; nonetheless, such differences between a prediction and a reference will have a significant impact on the textual similarity. This aspect can lead to textual-similarity metrics being too __strict__ when qualifying code.\n",
    "\n",
    "*Example of identical prediction and reference with different names of variables and function*\n",
    "\n",
    "![image info](./images/ai_code_errors/codebleu_example.PNG)\n",
    "\n",
    "___\n",
    "___\n",
    "___\n",
    "### No duplicate scripts section\n",
    "Given these results, it is safe to assume that the training of Logistic Regression models failed. This could be due to the highly unbalanced nature of the dataset: 77% of failing scripts. However, another aspect that could negatively impact the training is the presence of duplicate scripts among AI-generated code. Due to the \"nature\" of LLMs, the way it generates the response, it is possible to obtain the exact same answer for a given prompt, even across different models and temperatures. The presence of duplicate data could have a negative impact on the trained models -- a phenomenon known as *Overfitting* -- which is why the experimental protocol was repeated on a subset of the original dataset in which all the duplicate scripts are removed.\n",
    "\n",
    "___\n",
    "\n",
    "## Duplicate-free subset (to be deleted)\n",
    "\n",
    "Before continuing with the experimental protocol, it is worth noting that after the removal of duplicate scripts, the size of the obtained subset has shrunk to 1.42 million scripts, and its unbalanced nature is even more pronounced, with a fail rate of 87%."
   ],
   "id": "44393561e731e4ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Repeating the experimental protocol on the duplicate-free dataset\n",
    "\"\"\"\n",
    "distinct_dataset = 'ai_code_distinct'\n",
    "number_iterations = 50\n",
    "run_logistic_regression(distinct_dataset, number_iterations)\n",
    "logreg_average_variance(distinct_dataset, number_iterations)"
   ],
   "id": "a30c114629205f0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(distinct_dataset, metric=bleu)",
   "id": "f27c50e8cc05c783",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(distinct_dataset, metric=codebleu)",
   "id": "3d68b7f62ab3dcd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(distinct_dataset, metric=rouge)",
   "id": "ae62c2ecfe28f557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(distinct_dataset, metric=meteor)",
   "id": "d5a0e14d08dfb420",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_logreg_avg_var(distinct_dataset, metric=chrf)",
   "id": "404609087b6e472f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_confusion_matrix(distinct_dataset, nb_iterations=number_iterations)",
   "id": "cb9a26ac5cb36d30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![image info](./images/confusion_matrix/ai_code_distinct/bleu.png) ![image info](./images/confusion_matrix/ai_code_distinct/codebleu.png)\n",
    "![image info](./images/confusion_matrix/ai_code_distinct/rouge.png) ![image info](./images/confusion_matrix/ai_code_distinct/meteor.png)\n",
    "![image info](./images/confusion_matrix/ai_code_distinct/chrf.png)"
   ],
   "id": "ce092b6ccd2844b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
