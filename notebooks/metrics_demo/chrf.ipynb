{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ChrF\n",
    "\n",
    "[GitHub repository](https://github.com/huggingface/evaluate/tree/main/metrics/chrf) for more details.\n",
    "\n",
    "ChrF and ChrF++ are two MT evaluation metrics that use the F-score statistic for character n-gram matches. ChrF++ additionally includes word n-grams, which correlates more with human judgement. F-score is a measure of predictive performance; it is calculated from the *precision* and *recall*, similarly to __METEOR__."
   ],
   "id": "9e7a6513f42b5127"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:03:41.390980Z",
     "start_time": "2024-05-26T22:03:40.599380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import evaluate as ev\n",
    "\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    # A sorting key used to sort strings in a length-lexicographic order (length and alphabetical order)\n",
    "    return len(s), s\n",
    "\n",
    "\n",
    "def code_cleanup(script, remove_assert=False):\n",
    "    # Function that removes any unnecessary components of a given script (comments & tests), leaving only the code lines\n",
    "\n",
    "    # Removing the test component of HumanEval implementation following 'METADATA' information\n",
    "    if 'METADATA' in script:\n",
    "        script = script.split('METADATA', 1)[0]\n",
    "    elif 'def check(candidate)' in script:\n",
    "        script = script.split('def check(candidate)', 1)[0]\n",
    "\n",
    "    script_lines = script.splitlines()\n",
    "\n",
    "    multi_line_comment = False\n",
    "    comment_index = []\n",
    "    assert_index = []\n",
    "    empty_line_index = []\n",
    "\n",
    "    for index, line in enumerate(script_lines):\n",
    "\n",
    "        # Indexing any assert statement\n",
    "        if remove_assert and 'assert' in line and line[0] == 'a':\n",
    "            assert_index.append(index)\n",
    "            continue\n",
    "\n",
    "        if not multi_line_comment:\n",
    "            if '#' in line:\n",
    "                # Indexing single-line comments\n",
    "                if line.strip()[0] == '#':\n",
    "                    comment_index.append(index)\n",
    "                # Removing comment component of the line\n",
    "                else:\n",
    "                    cleaned_up_line = line.split('#', 1)[0]\n",
    "                    script_lines[index] = cleaned_up_line\n",
    "                continue\n",
    "\n",
    "            # Indexing the first line of multi-line comments\n",
    "            if '\"\"\"' in line or \"'''\" in line:\n",
    "                comment_index.append(index)\n",
    "                if line.count('\"\"\"') == 1 or line.count(\"'''\") == 1:\n",
    "                    multi_line_comment = True\n",
    "                continue\n",
    "\n",
    "        # Adding indexes for multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' not in line and \"'''\" not in line):\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing the last line of multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' in line or \"'''\" in line):\n",
    "            multi_line_comment = False\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing new lines and blank lines\n",
    "        if len(line) == 0 or line.isspace():\n",
    "            empty_line_index.append(index)\n",
    "            continue\n",
    "\n",
    "    # Merging indexes for comments, empty lines and assert statements\n",
    "    [comment_index.extend(indexes) for indexes in (empty_line_index, assert_index)]\n",
    "\n",
    "    # Removing all the unnecessary parts of code\n",
    "    for index in sorted(comment_index, reverse=True):\n",
    "        del script_lines[index]\n",
    "\n",
    "    # Concatenating the list of script lines\n",
    "    clean_script = '\\n'.join(script_lines)\n",
    "    return clean_script\n",
    "\n",
    "\n",
    "def chrf_metric(check_successful=False, check_failed=False, second_script=False, humaneval=False,\n",
    "                different_task=False):\n",
    "    \"\"\"\n",
    "    Function that applies the \"ChrF\" metric to the AI generated code\n",
    "    :param check_successful: if True, the chosen references are implementations with successful tests\n",
    "    :param check_failed: if True, the chosen references are implementations with failed tests/exec errors\n",
    "    :param second_script: choose a different good implementation as the baseline\n",
    "    :param humaneval: if True, the chosen baseline is the human-made implementation from \"HumanEval\" dataset\n",
    "    :param different_task: if True, the chosen references are the implementations for a different task (task 1)\n",
    "    :return the dictionary with all the scores, the average score as well as the variance\n",
    "    \"\"\"\n",
    "    if check_successful and check_failed:\n",
    "        print('Only one active parameter allowed between \"check_successful\" & \"check_failed\"')\n",
    "        exit(1)\n",
    "\n",
    "    json_path_prefix = '../../exp_results/metrics_calc'\n",
    "    funct_test_path_prefix = '../../exp_results/functionality_tests'\n",
    "\n",
    "    if humaneval:\n",
    "        baseline_script_path = '../../humaneval/000_has_close_elements.py'\n",
    "    elif second_script:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/42.py'\n",
    "    else:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/16.py'\n",
    "    baseline_file_name = baseline_script_path.split('/')[-1]\n",
    "\n",
    "    if different_task:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_1'\n",
    "    else:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0'\n",
    "\n",
    "    tested_task = data_folder_path.split('/')[-1]\n",
    "\n",
    "    task_name = 'HumanEval_0'\n",
    "    model_name = 'chatgpt'\n",
    "    model_temp = 'temp_0.8'\n",
    "\n",
    "    metric_name = 'chrf'\n",
    "    metric = ev.load(metric_name)\n",
    "\n",
    "    json_folder_path = os.path.join(json_path_prefix, metric_name, model_name, model_temp, task_name)\n",
    "\n",
    "    if not os.path.exists(json_folder_path):\n",
    "        os.makedirs(json_folder_path)\n",
    "\n",
    "    metric_dict = {'overall_score': 0, 'average_variance': 0}\n",
    "    file_names = []\n",
    "    overall_score = 0\n",
    "\n",
    "    with open(baseline_script_path, 'r') as f:\n",
    "        baseline = code_cleanup(f.read())\n",
    "\n",
    "    for path, folder, files in os.walk(data_folder_path):\n",
    "        for file_name in sorted(files, key=custom_sort_key):\n",
    "\n",
    "            # Avoiding comparison of the baseline to an identical prediction (i.e., comparing the baseline to the\n",
    "            # baseline)\n",
    "            if file_name == baseline_file_name:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                test_file_path = os.path.join(funct_test_path_prefix, model_name, model_temp, f'{tested_task}.json')\n",
    "                with open(test_file_path, 'r') as f:\n",
    "                    funct_dict = json.load(f)\n",
    "\n",
    "                # Filtering implementations with successful or failed tests\n",
    "                if check_successful:\n",
    "                    if not funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "                elif check_failed:\n",
    "                    if funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "\n",
    "                file_names.append(file_name)\n",
    "\n",
    "                current_script_path = os.path.join(path, file_name)\n",
    "                with open(current_script_path) as f:\n",
    "                    script = code_cleanup(f.read(), remove_assert=True)\n",
    "\n",
    "                results = metric.compute(predictions=[script], references=[baseline], word_order=2)\n",
    "\n",
    "                score = results['score']\n",
    "                metric_dict[file_name] = score\n",
    "                overall_score += score\n",
    "\n",
    "    nb_scripts = len(metric_dict.keys()) - 2\n",
    "\n",
    "    overall_score /= nb_scripts\n",
    "\n",
    "    metric_dict['overall_score'] = overall_score\n",
    "\n",
    "    return metric_dict"
   ],
   "id": "3bdcc0440204eaa7",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Applying __ChrF__ metric to code samples\n",
    "As per the experimental protocol, we start by choosing as the baseline the first *successful* implementation of __chatgpt_temp_0.8 task 0__ (script *16.py*) and compare it to all the other *good* implementations from this __model__ and __task__."
   ],
   "id": "8feef638790891b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:04:39.778968Z",
     "start_time": "2024-05-26T22:04:38.136366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrf_dict = chrf_metric(check_successful=True)\n",
    "\n",
    "for key, value in list(chrf_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "11f0a14dea37b5ba",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:05:20.227779Z",
     "start_time": "2024-05-26T22:05:19.060955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrf_dict = chrf_metric(check_successful=True, second_script=True)\n",
    "\n",
    "for key, value in list(chrf_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "c4c04db7ff171fbb",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparing with unsuccessful implementations\n",
    "Now we will take the first *good* implementation and compare it to scripts that *did not* pass the tests."
   ],
   "id": "7dc0fa453ad19926"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:06:23.922543Z",
     "start_time": "2024-05-26T22:06:22.612965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrf_dict = chrf_metric(check_failed=True)\n",
    "\n",
    "for key, value in list(chrf_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "9b874297c5b4acd9",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Human-made baseline implementation\n",
    "Now we'll consider as a baseline the human-written Eval+ implementation for __task 0__ and compare it to all the *correct* implementations generated by AI for the same task."
   ],
   "id": "59c020e51b9929c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:07:26.879938Z",
     "start_time": "2024-05-26T22:07:25.456480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrf_dict = chrf_metric(humaneval=True)\n",
    "\n",
    "for key, value in list(chrf_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "8b526c5fc0027adb",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Different task as a baseline\n",
    "Lastly, we will compare the standard baseline with the *successful* implementations for __task 1__ in order to see how this metric is affected by scripts that have drastically different __semantics__."
   ],
   "id": "86eab7bd4f1b3b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:08:12.230950Z",
     "start_time": "2024-05-26T22:08:10.416696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chrf_dict = chrf_metric(different_task=True)\n",
    "\n",
    "for key, value in list(chrf_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "bbff7e0a05ea6c09",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "8c28ab074183b455",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
