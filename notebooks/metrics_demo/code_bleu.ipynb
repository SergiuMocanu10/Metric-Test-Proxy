{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CodeBLEU\n",
    "\n",
    "[GitHub repository](https://github.com/k4black/codebleu) for more details.\n",
    "\n",
    "__CodeBLEU__ metric measures the __BLEU__ distance between code samples, as well as weighted n-grams (e.g., keywords from programming languages like: *def*, *int*, *list*, etc.), AST scores as well as data-flow scores. Considering the fact that this metric takes into consideration key-words that are native to programming languages, it can be applied exclusively to the metric-implemented languages: `Python`, `C`, `C#`, `C++`, `Java`, `JavaScript`, `PHP`, `Go`, `Ruby`, `Rust`.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Parameters\n",
    "The __CodeBLEU__ metric can be parameterized with 4 weights, one for each score (n-gram, weighted n-gram, AST and data_flow) with values between 0 and 1; the default value is *0.25*.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Output\n",
    "__CodeBLEU__ outputs the 4 aforementioned scores, as well as a global __codebleu__ score which is a mean value of the previous scores: &NewLine;\n",
    "\n",
    "    {\n",
    "    'codebleu': 0.5537, \n",
    "    'ngram_match_score': 0.1041, \n",
    "    'weighted_ngram_match_score': 0.1109, \n",
    "    'syntax_match_score': 1.0, \n",
    "    'dataflow_match_score': 1.0\n",
    "    }\n",
    "\n",
    "For a better analysis of the metric results, this implementation that applies the metric also measures the __variance__ for each score; therefore, every score outputs a tuple with 2 elements: the __score__ and the __variance__. Example:\n",
    "\n",
    "    {\n",
    "    'codebleu': (0.6386289761296269, 0.04037209526410621)\n",
    "    'ngram_match_score': (0.5727281562370962, 0.05641782117008404)\n",
    "    'weighted_ngram_match_score': (0.6557643564685458, 0.038337612508116244)\n",
    "    'syntax_match_score': (0.5423976608187137, 0.043705755617113116)\n",
    "    'dataflow_match_score': (0.7836257309941523, 0.054786088027085345)\n",
    "    }\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Issues\n",
    "The GitHub repository for this metric mentions an open issue with the __data-flow__ score; this might have an impact on the results of the experiment, but it is the best implementation of the __CodeBLEU__ metric available on the internet.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Important note\n",
    "For the sake of simplicity, the experiments using this metric are done with default parameters for the 4 scores. Further experiments with different parameters could yield more accurate results.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Usage example"
   ],
   "id": "b0a5d0a69afbbd77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T15:47:18.326199Z",
     "start_time": "2024-05-26T15:47:18.308562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from codebleu import calc_codebleu\n",
    "\n",
    "prediction = \"def add ( a , b ) :\\n return a + b\"\n",
    "reference = \"def sum ( first , second ) :\\n return second + first\"\n",
    "\n",
    "result = calc_codebleu([reference], [prediction], lang=\"python\")\n",
    "for key, value in result.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "810e30fbc531b797",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:07:02.386574Z",
     "start_time": "2024-05-26T18:07:02.350928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    # A sorting key used to sort strings in a length-lexicographic order (length and alphabetical order)\n",
    "    return len(s), s\n",
    "\n",
    "\n",
    "def code_cleanup(script, remove_assert=False):\n",
    "    # Function that removes any unnecessary components of a given script (comments & tests), leaving only the code lines\n",
    "\n",
    "    # Removing the test component of HumanEval implementation following 'METADATA' information\n",
    "    if 'METADATA' in script:\n",
    "        script = script.split('METADATA', 1)[0]\n",
    "    elif 'def check(candidate)' in script:\n",
    "        script = script.split('def check(candidate)', 1)[0]\n",
    "\n",
    "    script_lines = script.splitlines()\n",
    "\n",
    "    multi_line_comment = False\n",
    "    comment_index = []\n",
    "    assert_index = []\n",
    "    empty_line_index = []\n",
    "\n",
    "    for index, line in enumerate(script_lines):\n",
    "\n",
    "        # Indexing any assert statement\n",
    "        if remove_assert and 'assert' in line and line[0] == 'a':\n",
    "            assert_index.append(index)\n",
    "            continue\n",
    "\n",
    "        if not multi_line_comment:\n",
    "            if '#' in line:\n",
    "                # Indexing single-line comments\n",
    "                if line.strip()[0] == '#':\n",
    "                    comment_index.append(index)\n",
    "                # Removing comment component of the line\n",
    "                else:\n",
    "                    cleaned_up_line = line.split('#', 1)[0]\n",
    "                    script_lines[index] = cleaned_up_line\n",
    "                continue\n",
    "\n",
    "            # Indexing the first line of multi-line comments\n",
    "            if '\"\"\"' in line or \"'''\" in line:\n",
    "                comment_index.append(index)\n",
    "                if line.count('\"\"\"') == 1 or line.count(\"'''\") == 1:\n",
    "                    multi_line_comment = True\n",
    "                continue\n",
    "\n",
    "        # Adding indexes for multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' not in line and \"'''\" not in line):\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing the last line of multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' in line or \"'''\" in line):\n",
    "            multi_line_comment = False\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing new lines and blank lines\n",
    "        if len(line) == 0 or line.isspace():\n",
    "            empty_line_index.append(index)\n",
    "            continue\n",
    "\n",
    "    # Merging indexes for comments, empty lines and assert statements\n",
    "    [comment_index.extend(indexes) for indexes in (empty_line_index, assert_index)]\n",
    "\n",
    "    # Removing all the unnecessary parts of code\n",
    "    for index in sorted(comment_index, reverse=True):\n",
    "        del script_lines[index]\n",
    "\n",
    "    # Concatenating the list of script lines\n",
    "    clean_script = '\\n'.join(script_lines)\n",
    "    return clean_script\n",
    "\n",
    "\n",
    "def codebleu_metric(check_successful=False, check_failed=False, second_script=False, humaneval=False,\n",
    "                    different_task=False):\n",
    "    \"\"\"\n",
    "    Function that applies the \"CodeBLEU\" metric to the AI generated code\n",
    "    :param check_successful: if True, the chosen references are implementations with successful tests\n",
    "    :param check_failed: if True, the chosen references are implementations with failed tests/exec errors\n",
    "    :param second_script: choose a different good implementation as the baseline\n",
    "    :param humaneval: if True, the chosen baseline is the human-made implementation from \"HumanEval\" dataset\n",
    "    :param different_task: if True, the chosen references are the implementations for a different task (task 1)\n",
    "    :return the dictionary with all the scores, the average score as well as the variance\n",
    "    \"\"\"\n",
    "    if check_successful and check_failed:\n",
    "        print('Only one active parameter allowed between \"check_successful\" & \"check_failed\"')\n",
    "        exit(1)\n",
    "\n",
    "    json_path_prefix = '../../exp_results/metrics_calc'\n",
    "    funct_test_path_prefix = '../../exp_results/functionality_tests'\n",
    "\n",
    "    if humaneval:\n",
    "        baseline_script_path = '../../humaneval/000_has_close_elements.py'\n",
    "    elif second_script:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/42.py'\n",
    "    else:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/16.py'\n",
    "    baseline_file_name = baseline_script_path.split('/')[-1]\n",
    "\n",
    "    if different_task:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_1'\n",
    "    else:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0'\n",
    "\n",
    "    tested_task = data_folder_path.split('/')[-1]\n",
    "\n",
    "    task_name = 'HumanEval_0'\n",
    "    model_name = 'chatgpt'\n",
    "    model_temp = 'temp_0.8'\n",
    "\n",
    "    metric_name = 'codebleu'\n",
    "\n",
    "    json_folder_path = os.path.join(json_path_prefix, metric_name, model_name, model_temp, task_name)\n",
    "\n",
    "    if not os.path.exists(json_folder_path):\n",
    "        os.makedirs(json_folder_path)\n",
    "\n",
    "    metric_dict = {'codebleu': (0, 0),\n",
    "                   'ngram_match_score': (0, 0),\n",
    "                   'weighted_ngram_match_score': (0, 0),\n",
    "                   'syntax_match_score': (0, 0),\n",
    "                   'dataflow_match_score': (0, 0)}\n",
    "\n",
    "    file_names = []\n",
    "    overall_codebleu_score = 0\n",
    "    overall_ngram_score = 0\n",
    "    overall_weighted_ngram_score = 0\n",
    "    overall_syntax_score = 0\n",
    "    overall_dataflow_score = 0\n",
    "\n",
    "    with open(baseline_script_path, 'r') as f:\n",
    "        baseline = code_cleanup(f.read())\n",
    "\n",
    "    for path, folder, files in os.walk(data_folder_path):\n",
    "        for file_name in sorted(files, key=custom_sort_key):\n",
    "\n",
    "            # Avoiding comparison of the baseline to an identical prediction (i.e., comparing the baseline to the\n",
    "            # baseline)\n",
    "            if file_name == baseline_file_name:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                test_file_path = os.path.join(funct_test_path_prefix, model_name, model_temp, f'{tested_task}.json')\n",
    "                with open(test_file_path, 'r') as f:\n",
    "                    funct_dict = json.load(f)\n",
    "\n",
    "                # Filtering implementations with successful or failed tests\n",
    "                if check_successful:\n",
    "                    if not funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "                elif check_failed:\n",
    "                    if funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "\n",
    "                file_names.append(file_name)\n",
    "\n",
    "                current_script_path = os.path.join(path, file_name)\n",
    "                with open(current_script_path) as f:\n",
    "                    script = code_cleanup(f.read(), remove_assert=True)\n",
    "\n",
    "                results = calc_codebleu(predictions=[script], references=[baseline], lang='python')\n",
    "\n",
    "                metric_dict[file_name] = results\n",
    "                overall_codebleu_score += results['codebleu']\n",
    "                overall_ngram_score += results['ngram_match_score']\n",
    "                overall_weighted_ngram_score += results['weighted_ngram_match_score']\n",
    "                overall_syntax_score += results['syntax_match_score']\n",
    "                overall_dataflow_score += results['dataflow_match_score']\n",
    "\n",
    "    nb_scripts = len(metric_dict.keys()) - 5\n",
    "\n",
    "    overall_codebleu_score /= nb_scripts\n",
    "    overall_ngram_score /= nb_scripts\n",
    "    overall_weighted_ngram_score /= nb_scripts\n",
    "    overall_syntax_score /= nb_scripts\n",
    "    overall_dataflow_score /= nb_scripts\n",
    "\n",
    "    codebleu_variance = 0\n",
    "    ngram_variance = 0\n",
    "    weighted_ngram_variance = 0\n",
    "    syntax_variance = 0\n",
    "    dataflow_variance = 0\n",
    "\n",
    "    for file in file_names:\n",
    "        codebleu_variance += abs(overall_codebleu_score - metric_dict[file]['codebleu'])\n",
    "        ngram_variance += abs(overall_ngram_score - metric_dict[file]['ngram_match_score'])\n",
    "        weighted_ngram_variance += abs(overall_weighted_ngram_score - metric_dict[file]['weighted_ngram_match_score'])\n",
    "        syntax_variance += abs(overall_syntax_score - metric_dict[file]['syntax_match_score'])\n",
    "        dataflow_variance += abs(overall_dataflow_score - metric_dict[file]['dataflow_match_score'])\n",
    "\n",
    "    codebleu_variance /= nb_scripts\n",
    "    ngram_variance /= nb_scripts\n",
    "    weighted_ngram_variance /= nb_scripts\n",
    "    syntax_variance /= nb_scripts\n",
    "    dataflow_variance /= nb_scripts\n",
    "\n",
    "    metric_dict['codebleu'] = (overall_codebleu_score, codebleu_variance)\n",
    "    metric_dict['ngram_match_score'] = (overall_ngram_score, ngram_variance)\n",
    "    metric_dict['weighted_ngram_match_score'] = (overall_weighted_ngram_score, weighted_ngram_variance)\n",
    "    metric_dict['syntax_match_score'] = (overall_syntax_score, syntax_variance)\n",
    "    metric_dict['dataflow_match_score'] = (overall_dataflow_score, dataflow_variance)\n",
    "\n",
    "    return metric_dict"
   ],
   "id": "40bf794f242380b",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Applying __CodeBLEU__ metric to code samples\n",
    "As per the experimental protocol, we start by choosing as the baseline the first *successful* implementation of __chatgpt_temp_0.8 task 0__ (script *16.py*) and compare it to all the other *good* implementations from this __model__ and __task__."
   ],
   "id": "84b65db2d93fb368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:08:10.184505Z",
     "start_time": "2024-05-26T18:08:10.102142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "codebleu_dict = codebleu_metric(check_successful=True)\n",
    "\n",
    "for key, value in list(codebleu_dict.items())[:5]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "3ef09d16603bb595",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will consider the second *successful* script (*42.py*) as the baseline in order to analyze the difference in __CodeBLEU__ scores for different yet *correct* implementations of the same task.\n",
   "id": "7ea8c5c5b424b8ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:08:14.279077Z",
     "start_time": "2024-05-26T18:08:14.190670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "codebleu_dict = codebleu_metric(check_successful=True, second_script=True)\n",
    "\n",
    "for key, value in list(codebleu_dict.items())[:5]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "a496cb860baa2ff8",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can notice a slight difference in the overall score for these two __baselines__ (*0.642* for the script __16.py__ and *0.598* for the script __42.py__). The difference between the two scores is considerable yet smaller than the difference in score when applying the __BLEU__ metric, thanks to the __CodeBLEU__ metric taking into consideration not only the *textual similarities* but also other important factors.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Comparing with unsuccessful implementations\n",
    "Now we will take the first *good* implementation and compare it to scripts that *did not* pass the tests."
   ],
   "id": "1ef5deb8e4fbd01d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:15:01.741580Z",
     "start_time": "2024-05-26T18:15:01.687188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "codebleu_dict = codebleu_metric(check_failed=True)\n",
    "\n",
    "for key, value in list(codebleu_dict.items())[:5]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "3f5916b41b76b645",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can observe that, despite the overall score being lower than the first two, the difference in score compared to the previous experiment is equal to the difference in score for the first two experiments. However, if we pay closer attention to __syntax match__ and __data flow__ scores (__0.54 & 0.79__ and __0.58 & 0.74__ when comparing with *successful* implementations, in contrast with __0.47 & 0.65__ when comparing with *bad* implementations) we can discern considerably worse scores. Further experiments with the metric parameters might yield more coherent scores.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Human-made baseline implementation\n",
    "Now we'll consider as a baseline the human-written Eval+ implementation for __task 0__ and compare it to all the *correct* implementations generated by AI for the same task."
   ],
   "id": "699c81875a9db360"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:40:31.833801Z",
     "start_time": "2024-05-26T18:40:31.698389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "codebleu_dict = codebleu_metric(humaneval=True)\n",
    "\n",
    "for key, value in list(codebleu_dict.items())[:5]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "1b32b242d1558649",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As expected, the difference in the syntax and the presence of *assert* statements returns lower scores than any previous experiments. Nevertheless, if we are again to focus on __syntax match__ and __data flow__ scores, we can notice that these suffered a relatively smaller decrease in value compared to all the others, with the __syntax match__ score having the smallest degree of decline during current experiments.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Different task as a baseline\n",
    "Lastly, we will compare the standard baseline with the *successful* implementations for __task 1__ in order to see how this metric is affected by scripts that have drastically different __semantics__."
   ],
   "id": "27f0536880680681"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:56:53.511284Z",
     "start_time": "2024-05-26T18:56:53.179326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "codebleu_dict = codebleu_metric(check_successful=True, different_task=True)\n",
    "\n",
    "for key, value in list(codebleu_dict.items())[:5]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "475bb189e1043b8e",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As expected, the overall score is much lower than any previous experiment, except for the __data flow__ score; although very unusual, this could be explained by the similarities in the way the data is generally manipulated in simple programs (e.g., iterate over the list given as an input). The anomaly could also be due to the __issue__ mentioned in the GitHub repository. Further analysis is necessary.\n",
   "id": "8e8ccee8b9d8b3f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "9ec1ce27e944df7a",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
