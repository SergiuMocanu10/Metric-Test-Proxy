{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# METEOR\n",
    "[GitHub repository](https://github.com/huggingface/evaluate/tree/main/metrics/meteor) for more details\n",
    "\n",
    "__METEOR__ (Metric for Evaluation of Translation with Explicit ORdering) is a machine translation evaluation metric, which is calculated based on the harmonic mean of precision and recall, with recall weighted more than precision.\n",
    "\n",
    "In ML classification, precision and recall are two metrics that measure the quality of the predicted text: \n",
    "- __precision__ - among __predicted__ words, *how many of them are relevant*\n",
    "- __recall__ - among relevant words found in the __reference__, *how many of them are predicted*\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Parameters\n",
    "This metric has 3 optional parameters that are less intuitive for configuration. Values proposed in the research paper are used by default. For more information on the available parameters, see the [GitHub repository](https://github.com/huggingface/evaluate/tree/main/metrics/meteor).\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Usage example"
   ],
   "id": "490b2d448c4738bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:20:32.136021Z",
     "start_time": "2024-05-26T21:20:30.749335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "predictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\n",
    "references = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\n",
    "results = meteor.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ],
   "id": "1c66305b9e305772",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:30:39.945761Z",
     "start_time": "2024-05-26T21:30:39.929827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import evaluate as ev\n",
    "\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    # A sorting key used to sort strings in a length-lexicographic order (length and alphabetical order)\n",
    "    return len(s), s\n",
    "\n",
    "\n",
    "def code_cleanup(script, remove_assert=False):\n",
    "    # Function that removes any unnecessary components of a given script (comments & tests), leaving only the code lines\n",
    "\n",
    "    # Removing the test component of HumanEval implementation following 'METADATA' information\n",
    "    if 'METADATA' in script:\n",
    "        script = script.split('METADATA', 1)[0]\n",
    "    elif 'def check(candidate)' in script:\n",
    "        script = script.split('def check(candidate)', 1)[0]\n",
    "\n",
    "    script_lines = script.splitlines()\n",
    "\n",
    "    multi_line_comment = False\n",
    "    comment_index = []\n",
    "    assert_index = []\n",
    "    empty_line_index = []\n",
    "\n",
    "    for index, line in enumerate(script_lines):\n",
    "\n",
    "        # Indexing any assert statement\n",
    "        if remove_assert and 'assert' in line and line[0] == 'a':\n",
    "            assert_index.append(index)\n",
    "            continue\n",
    "\n",
    "        if not multi_line_comment:\n",
    "            if '#' in line:\n",
    "                # Indexing single-line comments\n",
    "                if line.strip()[0] == '#':\n",
    "                    comment_index.append(index)\n",
    "                # Removing comment component of the line\n",
    "                else:\n",
    "                    cleaned_up_line = line.split('#', 1)[0]\n",
    "                    script_lines[index] = cleaned_up_line\n",
    "                continue\n",
    "\n",
    "            # Indexing the first line of multi-line comments\n",
    "            if '\"\"\"' in line or \"'''\" in line:\n",
    "                comment_index.append(index)\n",
    "                if line.count('\"\"\"') == 1 or line.count(\"'''\") == 1:\n",
    "                    multi_line_comment = True\n",
    "                continue\n",
    "\n",
    "        # Adding indexes for multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' not in line and \"'''\" not in line):\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing the last line of multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' in line or \"'''\" in line):\n",
    "            multi_line_comment = False\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing new lines and blank lines\n",
    "        if len(line) == 0 or line.isspace():\n",
    "            empty_line_index.append(index)\n",
    "            continue\n",
    "\n",
    "    # Merging indexes for comments, empty lines and assert statements\n",
    "    [comment_index.extend(indexes) for indexes in (empty_line_index, assert_index)]\n",
    "\n",
    "    # Removing all the unnecessary parts of code\n",
    "    for index in sorted(comment_index, reverse=True):\n",
    "        del script_lines[index]\n",
    "\n",
    "    # Concatenating the list of script lines\n",
    "    clean_script = '\\n'.join(script_lines)\n",
    "    return clean_script\n",
    "\n",
    "\n",
    "def meteor_metric(check_successful=False, check_failed=False, second_script=False, humaneval=False,\n",
    "                  different_task=False):\n",
    "    \"\"\"\n",
    "    Function that applies the \"METEOR\" metric to the AI generated code\n",
    "    :param check_successful: if True, the chosen references are implementations with successful tests\n",
    "    :param check_failed: if True, the chosen references are implementations with failed tests/exec errors\n",
    "    :param second_script: choose a different good implementation as the baseline\n",
    "    :param humaneval: if True, the chosen baseline is the human-made implementation from \"HumanEval\" dataset\n",
    "    :param different_task: if True, the chosen references are the implementations for a different task (task 1)\n",
    "    :return the dictionary with all the scores, the average score as well as the variance\n",
    "    \"\"\"\n",
    "    if check_successful and check_failed:\n",
    "        print('Only one active parameter allowed between \"check_successful\" & \"check_failed\"')\n",
    "        exit(1)\n",
    "\n",
    "    json_path_prefix = '../../exp_results/metrics_calc'\n",
    "    funct_test_path_prefix = '../../exp_results/functionality_tests'\n",
    "\n",
    "    if humaneval:\n",
    "        baseline_script_path = '../../humaneval/000_has_close_elements.py'\n",
    "    elif second_script:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/42.py'\n",
    "    else:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/16.py'\n",
    "    baseline_file_name = baseline_script_path.split('/')[-1]\n",
    "\n",
    "    if different_task:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_1'\n",
    "    else:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0'\n",
    "\n",
    "    tested_task = data_folder_path.split('/')[-1]\n",
    "\n",
    "    task_name = 'HumanEval_0'\n",
    "    model_name = 'chatgpt'\n",
    "    model_temp = 'temp_0.8'\n",
    "\n",
    "    metric_name = 'meteor'\n",
    "    metric = ev.load(metric_name)\n",
    "\n",
    "    json_folder_path = os.path.join(json_path_prefix, metric_name, model_name, model_temp, task_name)\n",
    "\n",
    "    if not os.path.exists(json_folder_path):\n",
    "        os.makedirs(json_folder_path)\n",
    "\n",
    "    metric_dict = {'overall_score': 0, 'average_variance': 0}\n",
    "    file_names = []\n",
    "    overall_score = 0\n",
    "\n",
    "    with open(baseline_script_path, 'r') as f:\n",
    "        baseline = code_cleanup(f.read())\n",
    "\n",
    "    for path, folder, files in os.walk(data_folder_path):\n",
    "        for file_name in sorted(files, key=custom_sort_key):\n",
    "\n",
    "            # Avoiding comparison of the baseline to an identical prediction (i.e., comparing the baseline to the\n",
    "            # baseline)\n",
    "            if file_name == baseline_file_name:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                test_file_path = os.path.join(funct_test_path_prefix, model_name, model_temp, f'{tested_task}.json')\n",
    "                with open(test_file_path, 'r') as f:\n",
    "                    funct_dict = json.load(f)\n",
    "\n",
    "                # Filtering implementations with successful or failed tests\n",
    "                if check_successful:\n",
    "                    if not funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "                elif check_failed:\n",
    "                    if funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "\n",
    "                file_names.append(file_name)\n",
    "\n",
    "                current_script_path = os.path.join(path, file_name)\n",
    "                with open(current_script_path) as f:\n",
    "                    script = code_cleanup(f.read(), remove_assert=True)\n",
    "\n",
    "                results = metric.compute(predictions=[script], references=[baseline])\n",
    "\n",
    "                score = results['meteor']\n",
    "                metric_dict[file_name] = score\n",
    "                overall_score += score\n",
    "\n",
    "    nb_scripts = len(metric_dict.keys()) - 2\n",
    "\n",
    "    overall_score /= nb_scripts\n",
    "\n",
    "    metric_dict['overall_score'] = overall_score\n",
    "\n",
    "    variance = 0\n",
    "\n",
    "    for file in file_names:\n",
    "        variance += abs(overall_score - metric_dict[file])\n",
    "\n",
    "    metric_dict['average_variance'] = variance / nb_scripts\n",
    "\n",
    "    return metric_dict"
   ],
   "id": "d29b71fbc857cfcf",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Applying __METEOR__ metric to code samples\n",
    "As per the experimental protocol, we start by choosing as the baseline the first *successful* implementation of __chatgpt_temp_0.8 task 0__ (script *16.py*) and compare it to all the other *good* implementations from this __model__ and __task__."
   ],
   "id": "e3ad8a670a64ed99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:30:43.569689Z",
     "start_time": "2024-05-26T21:30:42.078921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meteor_dict = meteor_metric(check_successful=True)\n",
    "\n",
    "for key, value in list(meteor_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "5b48668ce8ac91d5",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Once again, the score obtained in the first stage of the experimental protocol is __higher__ than any previous score, even surpassing the __ROUGE's__ *0.81* score.\n",
    "\n",
    "Now we will consider the second *successful* script (*42.py*) as the baseline in order to analyze the difference in __CodeBLEU__ scores for different yet *correct* implementations of the same task."
   ],
   "id": "5afe2febafd496c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:31:00.160472Z",
     "start_time": "2024-05-26T21:30:58.895137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meteor_dict = meteor_metric(check_successful=True, second_script=True)\n",
    "\n",
    "for key, value in list(meteor_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "88890b76b4da1614",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparing with unsuccessful implementations\n",
    "Now we will take the first *good* implementation and compare it to scripts that *did not* pass the tests."
   ],
   "id": "2d7135c80332de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:06:37.582529Z",
     "start_time": "2024-05-26T22:06:36.122293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meteor_dict = meteor_metric(check_failed=True)\n",
    "\n",
    "for key, value in list(meteor_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "883034eb702745c1",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Human-made baseline implementation\n",
    "Now we'll consider as a baseline the human-written Eval+ implementation for __task 0__ and compare it to all the *correct* implementations generated by AI for the same task."
   ],
   "id": "169a691f727536ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:36:37.921768Z",
     "start_time": "2024-05-26T21:36:36.463430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meteor_dict = meteor_metric(humaneval=True)\n",
    "\n",
    "for key, value in list(meteor_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "a8cc93f567d19ab2",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Different task as a baseline\n",
    "Lastly, we will compare the standard baseline with the *successful* implementations for __task 1__ in order to see how this metric is affected by scripts that have drastically different __semantics__."
   ],
   "id": "41b193f0ab89b621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:36:30.970094Z",
     "start_time": "2024-05-26T21:36:28.813086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meteor_dict = meteor_metric(different_task=True)\n",
    "\n",
    "for key, value in list(meteor_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "8794a580ebfaef2a",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "16be5a6ae8834da8",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
