{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RougeL\n",
    "[GitHub repository](https://github.com/huggingface/evaluate/tree/main/metrics/rouge) for more details.\n",
    "\n",
    "__Rouge__ (Recall-Oriented Understudy for Gisting Evaluation) is a metric used for evaluating automatic summarization and machine-translated texts. In ML classification, __recall__ is a metric that measures *how much of the __relevant__ information* was detected by the model.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Output\n",
    "__ROUGE__ metric outputs 4 scores: \n",
    "  - `rouge1` - 1-gram based score\n",
    "  - `rouge2` - 2-gram based score\n",
    "  - `rougeL` - longest n-gram score\n",
    "  - `rougeLSum` - same as `rougeL` but splits the text based on '\\n'\n",
    "\n",
    "For the purpose of *code quality* evaluation, `rougeL` seems the most relevant score.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Usage example"
   ],
   "id": "f83ba6fe4d33517c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:22:51.767034Z",
     "start_time": "2024-05-26T20:22:50.464870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ],
   "id": "c6febe456f489f0b",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:32:52.420875Z",
     "start_time": "2024-05-26T20:32:52.402924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import evaluate as ev\n",
    "\n",
    "\n",
    "def custom_sort_key(s):\n",
    "    # A sorting key used to sort strings in a length-lexicographic order (length and alphabetical order)\n",
    "    return len(s), s\n",
    "\n",
    "\n",
    "def code_cleanup(script, remove_assert=False):\n",
    "    # Function that removes any unnecessary components of a given script (comments & tests), leaving only the code lines\n",
    "\n",
    "    # Removing the test component of HumanEval implementation following 'METADATA' information\n",
    "    if 'METADATA' in script:\n",
    "        script = script.split('METADATA', 1)[0]\n",
    "    elif 'def check(candidate)' in script:\n",
    "        script = script.split('def check(candidate)', 1)[0]\n",
    "\n",
    "    script_lines = script.splitlines()\n",
    "\n",
    "    multi_line_comment = False\n",
    "    comment_index = []\n",
    "    assert_index = []\n",
    "    empty_line_index = []\n",
    "\n",
    "    for index, line in enumerate(script_lines):\n",
    "\n",
    "        # Indexing any assert statement\n",
    "        if remove_assert and 'assert' in line and line[0] == 'a':\n",
    "            assert_index.append(index)\n",
    "            continue\n",
    "\n",
    "        if not multi_line_comment:\n",
    "            if '#' in line:\n",
    "                # Indexing single-line comments\n",
    "                if line.strip()[0] == '#':\n",
    "                    comment_index.append(index)\n",
    "                # Removing comment component of the line\n",
    "                else:\n",
    "                    cleaned_up_line = line.split('#', 1)[0]\n",
    "                    script_lines[index] = cleaned_up_line\n",
    "                continue\n",
    "\n",
    "            # Indexing the first line of multi-line comments\n",
    "            if '\"\"\"' in line or \"'''\" in line:\n",
    "                comment_index.append(index)\n",
    "                if line.count('\"\"\"') == 1 or line.count(\"'''\") == 1:\n",
    "                    multi_line_comment = True\n",
    "                continue\n",
    "\n",
    "        # Adding indexes for multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' not in line and \"'''\" not in line):\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing the last line of multi-line comments\n",
    "        if multi_line_comment and ('\"\"\"' in line or \"'''\" in line):\n",
    "            multi_line_comment = False\n",
    "            comment_index.append(index)\n",
    "            continue\n",
    "\n",
    "        # Indexing new lines and blank lines\n",
    "        if len(line) == 0 or line.isspace():\n",
    "            empty_line_index.append(index)\n",
    "            continue\n",
    "\n",
    "    # Merging indexes for comments, empty lines and assert statements\n",
    "    [comment_index.extend(indexes) for indexes in (empty_line_index, assert_index)]\n",
    "\n",
    "    # Removing all the unnecessary parts of code\n",
    "    for index in sorted(comment_index, reverse=True):\n",
    "        del script_lines[index]\n",
    "\n",
    "    # Concatenating the list of script lines\n",
    "    clean_script = '\\n'.join(script_lines)\n",
    "    return clean_script\n",
    "\n",
    "\n",
    "def rouge_metric(check_successful=False, check_failed=False, second_script=False, humaneval=False,\n",
    "                 different_task=False):\n",
    "    \"\"\"\n",
    "    Function that applies the \"RougeL\" metric to the AI generated code\n",
    "    :param check_successful: if True, the chosen references are implementations with successful tests\n",
    "    :param check_failed: if True, the chosen references are implementations with failed tests/exec errors\n",
    "    :param second_script: choose a different good implementation as the baseline\n",
    "    :param humaneval: if True, the chosen baseline is the human-made implementation from \"HumanEval\" dataset\n",
    "    :param different_task: if True, the chosen references are the implementations for a different task (task 1)\n",
    "    :return the dictionary with all the scores, the average score as well as the variance\n",
    "    \"\"\"\n",
    "    if check_successful and check_failed:\n",
    "        print('Only one active parameter allowed between \"check_successful\" & \"check_failed\"')\n",
    "        exit(1)\n",
    "\n",
    "    json_path_prefix = '../../exp_results/metrics_calc'\n",
    "    funct_test_path_prefix = '../../exp_results/functionality_tests'\n",
    "\n",
    "    if humaneval:\n",
    "        baseline_script_path = '../../humaneval/000_has_close_elements.py'\n",
    "    elif second_script:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/42.py'\n",
    "    else:\n",
    "        baseline_script_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0/16.py'\n",
    "    baseline_file_name = baseline_script_path.split('/')[-1]\n",
    "\n",
    "    if different_task:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_1'\n",
    "    else:\n",
    "        data_folder_path = '../../../ai_code/chatgpt_temp_0.8/HumanEval_0'\n",
    "\n",
    "    tested_task = data_folder_path.split('/')[-1]\n",
    "\n",
    "    task_name = 'HumanEval_0'\n",
    "    model_name = 'chatgpt'\n",
    "    model_temp = 'temp_0.8'\n",
    "\n",
    "    metric_name = 'rouge'\n",
    "    metric = ev.load(metric_name)\n",
    "\n",
    "    json_folder_path = os.path.join(json_path_prefix, metric_name, model_name, model_temp, task_name)\n",
    "\n",
    "    if not os.path.exists(json_folder_path):\n",
    "        os.makedirs(json_folder_path)\n",
    "\n",
    "    metric_dict = {'overall_score': 0, 'average_variance': 0}\n",
    "    file_names = []\n",
    "    overall_score = 0\n",
    "\n",
    "    with open(baseline_script_path, 'r') as f:\n",
    "        baseline = code_cleanup(f.read())\n",
    "\n",
    "    for path, folder, files in os.walk(data_folder_path):\n",
    "        for file_name in sorted(files, key=custom_sort_key):\n",
    "\n",
    "            # Avoiding comparison of the baseline to an identical prediction (i.e., comparing the baseline to the\n",
    "            # baseline)\n",
    "            if file_name == baseline_file_name:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                test_file_path = os.path.join(funct_test_path_prefix, model_name, model_temp, f'{tested_task}.json')\n",
    "                with open(test_file_path, 'r') as f:\n",
    "                    funct_dict = json.load(f)\n",
    "\n",
    "                # Filtering implementations with successful or failed tests\n",
    "                if check_successful:\n",
    "                    if not funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "                elif check_failed:\n",
    "                    if funct_dict[file_name]['successful']:\n",
    "                        continue\n",
    "\n",
    "                file_names.append(file_name)\n",
    "\n",
    "                current_script_path = os.path.join(path, file_name)\n",
    "                with open(current_script_path) as f:\n",
    "                    script = code_cleanup(f.read(), remove_assert=True)\n",
    "\n",
    "                results = metric.compute(predictions=[script], references=[baseline])\n",
    "\n",
    "                score = results['rougeL']\n",
    "                metric_dict[file_name] = score\n",
    "                overall_score += score\n",
    "\n",
    "    nb_scripts = len(metric_dict.keys()) - 2\n",
    "\n",
    "    overall_score /= nb_scripts\n",
    "\n",
    "    metric_dict['overall_score'] = overall_score\n",
    "\n",
    "    variance = 0\n",
    "\n",
    "    for file in file_names:\n",
    "        variance += abs(overall_score - metric_dict[file])\n",
    "\n",
    "    metric_dict['average_variance'] = variance / nb_scripts\n",
    "\n",
    "    return metric_dict"
   ],
   "id": "9ff1bc930563cc15",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Applying __ROUGE__ metric to code samples\n",
    "As per the experimental protocol, we start by choosing as the baseline the first *successful* implementation of __chatgpt_temp_0.8 task 0__ (script *16.py*) and compare it to all the other *good* implementations from this __model__ and __task__."
   ],
   "id": "73c26f9a5a5196c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:32:58.506068Z",
     "start_time": "2024-05-26T20:32:54.831256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rouge_dict = rouge_metric(check_successful=True)\n",
    "\n",
    "for key, value in list(rouge_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "67b822b667a9e506",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So far, this metric yields highest overall score for the first step of the experimental protocol (compared to __BLEU__ and __CodeBLEU__ metric).\n",
    "\n",
    "Now we will consider the second *successful* script (*42.py*) as the baseline in order to analyze the difference in __CodeBLEU__ scores for different yet *correct* implementations of the same task."
   ],
   "id": "1eb781dc87c4ba35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:33:59.595087Z",
     "start_time": "2024-05-26T20:33:55.588898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rouge_dict = rouge_metric(check_successful=True, second_script=True)\n",
    "\n",
    "for key, value in list(rouge_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "d8c294d3bb37ca02",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As per usually, comparing the score of two *good* baselines returns different scores.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Comparing with unsuccessful implementations\n",
    "Now we will take the first *good* implementation and compare it to scripts that *did not* pass the tests."
   ],
   "id": "4e797fc3daeb5537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:39:11.731498Z",
     "start_time": "2024-05-26T20:39:09.101031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rouge_dict = rouge_metric(check_failed=True)\n",
    "\n",
    "for key, value in list(rouge_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "9a37e4f0a620d6a2",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Similarly to the __BLEU__ metric, the comparison with *failed* implementations returns scores similar to those obtained when comparing to the *successful* ones.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Human-made baseline implementation\n",
    "Now we'll consider as a baseline the human-written Eval+ implementation for __task 0__ and compare it to all the *correct* implementations generated by AI for the same task."
   ],
   "id": "a556e4dd4588c0dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:49:25.739063Z",
     "start_time": "2024-05-26T20:49:20.734224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rouge_dict = rouge_metric(humaneval=True)\n",
    "\n",
    "for key, value in list(rouge_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "ce63bc768efaeb02",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As observed before, such factors like the presence of *assert* statements in the __human-written__ code yields lower similarity score.\n",
    "\n",
    "&NewLine;\n",
    "\n",
    "## Different task as a baseline\n",
    "Lastly, we will compare the standard baseline with the *successful* implementations for __task 1__ in order to see how this metric is affected by scripts that have drastically different __semantics__."
   ],
   "id": "1591cecfeb933f55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:55:41.305846Z",
     "start_time": "2024-05-26T20:55:14.024944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rouge_dict = rouge_metric(different_task=True)\n",
    "\n",
    "for key, value in list(rouge_dict.items())[:2]:\n",
    "    print(f'{key}: {value}')"
   ],
   "id": "27d26f7b97e9b6b9",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As expected, comparing with scripts that implement a __different functionality__ outputs lower scores. One important thing to note is that the decrease observed in this experiment is much more substantial than anything seen in the previous experiments; this might be due to the *__recall-oriented__* nature of __ROUGE__: the metric measures how much of the *relevant* information is kept and, since little to no similarities are present in the two task implementations, the score decay is much more significant.\n",
   "id": "c6a51d1dc698f855"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "e830136376d66143",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
